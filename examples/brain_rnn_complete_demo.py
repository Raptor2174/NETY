#!/usr/bin/env python3
"""
D√©monstration compl√®te : Brain + RNN + Knowledge Base

Ce script montre comment NETY peut :
1. Recevoir une question
2. R√©cup√©rer le contexte de la base de connaissances (RAG)
3. Traiter avec le RNN
4. G√©n√©rer une r√©ponse
"""

import torch
from nety.knowledge_base import init_databases, KnowledgeManager, SearchEngine
from nety.modules.text.modele_rnn import ModeleRNN
from nety.modules.text.tokenizer import SimpleTokenizer


class NETYBrainWithRNN:
    """Brain NETY int√©grant RNN et Knowledge Base"""
    
    def __init__(self):
        print("=" * 70)
        print(" üß† Initialisation NETY Brain + RNN + Knowledge Base")
        print("=" * 70)
        
        # 1. Initialize Knowledge Base
        print("\nüìä Initialisation de la base de connaissances...")
        init_databases()
        self.km = KnowledgeManager()
        self.search = SearchEngine()
        
        # 2. Initialize RNN
        print("ü§ñ Initialisation du mod√®le RNN...")
        self.rnn_model = ModeleRNN(
            input_size=1,
            hidden_size=64,
            output_size=10,  # 10 classes de r√©ponses possibles
            num_layers=2
        )
        
        # 3. Initialize Tokenizer
        print("üìù Initialisation du tokenizer...")
        self.tokenizer = SimpleTokenizer(vocab_size=2000)
        
        # 4. Load initial knowledge
        self._load_initial_knowledge()
        
        print("\n‚úÖ Initialisation termin√©e!\n")
    
    def _load_initial_knowledge(self):
        """Charge des connaissances de base"""
        print("\nüìö Chargement des connaissances de base...")
        
        knowledge_base = [
            {
                "title": "Intelligence Artificielle",
                "content": "L'intelligence artificielle (IA) est la simulation de processus d'intelligence humaine par des machines",
                "category": "ia"
            },
            {
                "title": "RNN",
                "content": "Les r√©seaux de neurones r√©currents (RNN) sont adapt√©s pour traiter des s√©quences de donn√©es",
                "category": "deep_learning"
            },
            {
                "title": "Python",
                "content": "Python est un langage de programmation interpr√©t√©, orient√© objet et de haut niveau",
                "category": "programmation"
            },
            {
                "title": "NETY",
                "content": "NETY est une IA multimodale capable de traiter du texte, des images et de l'audio",
                "category": "projet"
            }
        ]
        
        # Check if already loaded
        stats = self.km.get_stats()
        if stats['knowledge_count'] >= len(knowledge_base):
            print(f"   ‚úÖ {stats['knowledge_count']} connaissances d√©j√† charg√©es")
            return
        
        # Add knowledge
        for kb in knowledge_base:
            self.km.add_knowledge(**kb)
        
        print(f"   ‚úÖ {len(knowledge_base)} connaissances charg√©es")
        
        # Train tokenizer on all knowledge
        all_texts = [kb["content"] for kb in knowledge_base]
        self.tokenizer.fit(all_texts)
    
    def process_question(self, question: str) -> dict:
        """
        Traite une question avec RAG + RNN
        
        Returns:
            dict avec context, rnn_output, et answer
        """
        print(f"\n{'='*70}")
        print(f"‚ùì Question: {question}")
        print(f"{'='*70}")
        
        # 1. Retrieve context from KB
        print("\nüìö √âtape 1: R√©cup√©ration du contexte...")
        context = self.search.get_context_for_query(question, max_results=2)
        
        if context:
            print(f"   ‚úÖ Contexte trouv√© ({len(context)} caract√®res)")
            print(f"   Aper√ßu: {context[:100]}...")
        else:
            print("   ‚ö†Ô∏è  Aucun contexte trouv√©")
            context = ""
        
        # 2. Prepare input for RNN
        print("\nüîß √âtape 2: Pr√©paration pour le RNN...")
        combined_input = f"{question} {context}"
        
        # Tokenize
        encoded = self.tokenizer.encode(combined_input[:200], max_length=30)
        print(f"   ‚úÖ Texte tokenis√©: {encoded.shape}")
        
        # Reshape for RNN: (batch=1, seq_len=30, input_size=1)
        x = encoded.unsqueeze(0).unsqueeze(-1).float()
        
        # 3. Process with RNN
        print("\nü§ñ √âtape 3: Traitement par le RNN...")
        with torch.no_grad():
            rnn_output = self.rnn_model(x)
        
        # Get predicted class
        predicted_class = torch.argmax(rnn_output, dim=1).item()
        confidence = torch.softmax(rnn_output, dim=1).max().item()
        
        print(f"   ‚úÖ Classe pr√©dite: {predicted_class}")
        print(f"   ‚úÖ Confiance: {confidence:.2%}")
        
        # 4. Generate answer (simulation)
        print("\nüí¨ √âtape 4: G√©n√©ration de la r√©ponse...")
        
        # Dans une vraie impl√©mentation, on utiliserait un d√©codeur
        # Ici on simule avec le contexte
        if context:
            answer = f"D'apr√®s mes connaissances : {context[:150]}..."
        else:
            answer = "Je n'ai pas assez d'informations pour r√©pondre."
        
        print(f"   ‚úÖ R√©ponse g√©n√©r√©e")
        
        return {
            "question": question,
            "context": context,
            "rnn_output": rnn_output,
            "predicted_class": predicted_class,
            "confidence": confidence,
            "answer": answer
        }
    
    def interactive_mode(self):
        """Mode interactif"""
        print("\n" + "="*70)
        print(" üéÆ Mode Interactif NETY")
        print("="*70)
        print("\nTapez 'exit' pour quitter\n")
        
        while True:
            try:
                question = input("üí≠ Vous: ")
                
                if question.lower() in ['exit', 'quit', 'q']:
                    print("\nüëã Au revoir!")
                    break
                
                if not question.strip():
                    continue
                
                result = self.process_question(question)
                
                print(f"\nü§ñ NETY: {result['answer']}\n")
                
            except KeyboardInterrupt:
                print("\n\nüëã Au revoir!")
                break


def main():
    """Demo principale"""
    
    # Create NETY Brain
    nety = NETYBrainWithRNN()
    
    # Demo questions
    demo_questions = [
        "Qu'est-ce que l'intelligence artificielle?",
        "Parle-moi des RNN",
        "C'est quoi Python?",
        "Qui est NETY?"
    ]
    
    print("\n" + "="*70)
    print(" üéØ D√©monstration avec Questions Pr√©d√©finies")
    print("="*70)
    
    for question in demo_questions:
        result = nety.process_question(question)
        
        print(f"\n{'‚îÄ'*70}")
        print(f"ü§ñ NETY r√©pond:")
        print(f"{'‚îÄ'*70}")
        print(result['answer'])
        print()
        
        input("Appuyez sur Entr√©e pour continuer...")
    
    # Interactive mode
    print("\n" + "="*70)
    print(" Voulez-vous passer en mode interactif? (y/n)")
    print("="*70)
    
    choice = input("Votre choix: ")
    if choice.lower() in ['y', 'yes', 'o', 'oui']:
        nety.interactive_mode()


if __name__ == "__main__":
    main()
