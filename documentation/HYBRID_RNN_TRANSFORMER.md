# Architecture Hybride RNN-Transformer

## üéØ Objectif

Combiner la puissance du **RNN existant** (3.5M param√®tres) comme encodeur avec un **Mini-Transformer Decoder** (6 couches, 512 dims) pour une g√©n√©ration de texte autoregressive de haute qualit√©.

## üèóÔ∏è Architecture

```
Input Message (tokens)
    ‚Üì
Embedding Layer (vocab ‚Üí 512 dims)
    ‚Üì
RNN Encoder (ModeleRNN - 3 couches, bi-directionnel, attention)
    ‚Üì 
Contexte encod√© (batch, 1, 512)
    ‚Üì
Mini-Transformer Decoder (6 couches, 8 t√™tes, 512 dims)
    ‚Üì
G√©n√©ration autoregressive (top-k, top-p sampling)
    ‚Üì
Response Text
```

**Inspir√© de:** BART, T5, MarianMT (architectures Encoder-Decoder)

## üìä Statistiques du Mod√®le

| Composant | Param√®tres | Pourcentage |
|-----------|------------|-------------|
| **RNN Encoder** | ~6M | 18.4% |
| **Transformer Decoder** | ~26M | 80.1% |
| **Total** | **~32.8M** | 100% |

**Taille m√©moire:** ~125 MB (Float32)

## üîß Composants

### 1. RNN Encoder (`ModeleRNN`)
- **Architecture:** LSTM bi-directionnel avec 3 couches
- **Attention:** Multi-head attention (4 t√™tes)
- **Input:** Embeddings (512 dims)
- **Output:** Vecteur contextualis√© (512 dims)
- **R√¥le:** Comprendre le message d'entr√©e et cr√©er une repr√©sentation s√©mantique

### 2. Transformer Decoder (`MiniTransformerDecoder`)
- **Couches:** 6 couches transformer
- **T√™tes d'attention:** 8 t√™tes
- **Dimension mod√®le:** 512
- **Feedforward:** 2048 dims
- **R√¥le:** G√©n√©rer la r√©ponse token par token en utilisant le contexte du RNN

### 3. Architecture Hybride (`HybridRNNTransformer`)
- **Int√©gration:** Combine RNN encoder + Transformer decoder
- **G√©n√©ration:** Autoregressive avec top-k et nucleus (top-p) sampling
- **Flexibilit√©:** Temperature control pour la diversit√©

## üí° Utilisation

### Exemple de base

```python
import torch
from nety.modules.text.transformer_decoder import HybridRNNTransformer

# Cr√©er le mod√®le
model = HybridRNNTransformer(
    vocab_size=1000,
    rnn_hidden_size=256,
    rnn_num_layers=3,
    decoder_d_model=512,
    decoder_nhead=8,
    decoder_num_layers=6,
    dropout=0.1
)

# Pr√©parer les tokens d'entr√©e (exemple)
src_tokens = torch.LongTensor([[4, 7, 8, 9]])  # "bonjour comment vas tu"

# G√©n√©rer une r√©ponse
response_tokens = model.generate(
    src=src_tokens,
    start_token=1,  # <sos>
    end_token=2,    # <eos>
    max_length=50,
    temperature=0.8,
    top_k=50,
    top_p=0.9
)

print(f"Response tokens: {response_tokens}")
```

### Forward pass avec teacher forcing

```python
# Pour l'entra√Ænement
src = torch.LongTensor([[4, 7, 8, 9]])  # Input message
tgt = torch.LongTensor([[5, 10, 15, 20]])  # Target response

# Forward pass
logits = model(src, tgt)  # (batch, seq_len, vocab_size)

# Calculer la loss
loss_fn = torch.nn.CrossEntropyLoss()
loss = loss_fn(logits.view(-1, vocab_size), tgt.view(-1))
```

## üöÄ Avantages de l'Architecture

### 1. **Meilleur que RNN seul**
- ‚úÖ Le Transformer decoder capture mieux les d√©pendances √† long terme
- ‚úÖ Attention multi-t√™te pour un contexte riche
- ‚úÖ G√©n√©ration plus coh√©rente et naturelle

### 2. **Plus l√©ger que Transformer pur**
- ‚úÖ RNN encoder r√©utilise le mod√®le existant (3.5M params)
- ‚úÖ Seulement 32.8M params vs 100M+ pour GPT-2 small
- ‚úÖ Rapide sur CPU/GPU

### 3. **Flexibilit√©**
- ‚úÖ Temperature pour contr√¥ler la cr√©ativit√©
- ‚úÖ Top-k et top-p sampling pour √©viter la r√©p√©tition
- ‚úÖ Compatible avec le syst√®me √©motionnel (limbic filter)

## üìà Comparaison avec d'autres architectures

| Architecture | Params | Avantages | Inconv√©nients |
|--------------|--------|-----------|---------------|
| **RNN seul** | 3.5M | Rapide, l√©ger | G√©n√©ration limit√©e |
| **Hybrid RNN-Transformer** | **32.8M** | **√âquilibr√©, performant** | **N√©cessite entra√Ænement** |
| Transformer pur (GPT-2) | 117M | Tr√®s performant | Lourd, lent |

## üéì Sampling Strategies

### Temperature
- **0.1-0.5:** R√©ponses conservatrices et pr√©visibles
- **0.7-0.9:** √âquilibr√© (recommand√©)
- **1.0-2.0:** Cr√©atif et diversifi√©

### Top-K Sampling
- Garde les K tokens les plus probables
- `top_k=50` : bon √©quilibre
- `top_k=0` : d√©sactiv√©

### Nucleus (Top-P) Sampling
- Garde les tokens jusqu'√† cumul de probabilit√© = P
- `top_p=0.9` : recommand√©
- `top_p=0.0` : d√©sactiv√©

## üî¨ Test et D√©mo

Ex√©cuter le script de d√©monstration :

```bash
python tests/demo_hybrid_rnn_transformer.py
```

**R√©sultat attendu:**
- ‚úÖ Initialisation du mod√®le
- ‚úÖ 4 tests de g√©n√©ration
- ‚úÖ Test forward pass complet
- ‚úÖ Statistiques du mod√®le

## üõ†Ô∏è Int√©gration avec RNNResponseGenerator

L'architecture hybride est automatiquement utilis√©e dans `RNNResponseGenerator` :

```python
from nety.core.rnn_response_generator import RNNResponseGenerator

# Le g√©n√©rateur utilise maintenant l'architecture hybride
generator = RNNResponseGenerator()

# G√©n√©rer une r√©ponse
response = generator.generate(
    message="Bonjour, comment vas-tu ?",
    context={"user_profile": {"name": "Alice"}},
    max_length=50,
    temperature=0.8
)

print(response)
```

## üìù Notes sur l'Entra√Ænement

‚ö†Ô∏è **Le mod√®le n'est pas encore entra√Æn√©.** Pour l'instant, il g√©n√®re des tokens al√©atoires.

Pour entra√Æner le mod√®le :

1. **Pr√©parer un dataset** de paires (question, r√©ponse)
2. **Tokenizer** les textes avec le vocabulaire
3. **Entra√Æner avec teacher forcing** :
   ```python
   optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
   loss_fn = torch.nn.CrossEntropyLoss(ignore_index=0)  # <pad>
   
   for epoch in range(num_epochs):
       for src, tgt in dataloader:
           # Forward
           logits = model(src, tgt[:, :-1])
           
           # Loss
           loss = loss_fn(
               logits.view(-1, vocab_size),
               tgt[:, 1:].reshape(-1)
           )
           
           # Backward
           optimizer.zero_grad()
           loss.backward()
           optimizer.step()
   ```

4. **Sauvegarder le mod√®le** :
   ```python
   torch.save(model.state_dict(), "hybrid_model.pt")
   ```

## üéØ Prochaines √âtapes

- [ ] Entra√Æner sur un dataset de conversations
- [ ] Int√©grer avec le syst√®me de m√©moire
- [ ] Ajouter beam search pour une meilleure g√©n√©ration
- [ ] Fine-tuning avec des donn√©es sp√©cifiques √† NETY
- [ ] Optimisation pour CPU (quantization, pruning)

## üìö R√©f√©rences

- **Attention Is All You Need** (Vaswani et al., 2017)
- **BART:** Denoising Sequence-to-Sequence Pre-training (Lewis et al., 2019)
- **T5:** Text-to-Text Transfer Transformer (Raffel et al., 2020)

---

**Cr√©√© le:** 3 f√©vrier 2026  
**Auteur:** GitHub Copilot & Raptor_  
**Projet:** NETY - Neural Engine for Textual Yields
