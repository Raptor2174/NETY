# ðŸ§  Cortex Textuel RNN - Documentation Technique

## Vue d'ensemble

Le **Cortex Textuel Autonome** est le nouveau rÃ©seau neuronal du cortex cÃ©rÃ©bral (textuel) de NETY. C'est une version modernisÃ©e et amÃ©liorÃ©e du vieux RNN original, transformÃ©e en cÅ“ur neuronal autonome du systÃ¨me NETY.

### CaractÃ©ristiques Principales

âœ¨ **Architecture AvancÃ©e:**
- **LSTM Bi-directionnel** (3 couches) : Traite le texte dans les deux sens pour une meilleure comprÃ©hension contextuelle
- **Attention Multi-tÃªte** (4 tÃªtes) : Focalise l'attention sur les parties pertinentes du texte
- **Batch Normalization** : Stabilise l'entraÃ®nement et l'infÃ©rence
- **Dropout** : RÃ©duit l'overfitting (30%)
- **Activation GELU** : Activation moderne et progressive

ðŸ§  **Ã‰tat Persistant Autonome:**
- **MÃ©moire Court-terme** : L'Ã©tat cachÃ© (h, c) persiste entre les interactions
- **Historique d'Ã‰tat** : Les 50 derniers Ã©tats cachÃ©s sont conservÃ©s
- **Contexte Glissant** : FenÃªtre des 20 derniÃ¨res interactions maintenue en mÃ©moire

ðŸ’­ **IntÃ©gration Limbique:**
- **Modulation Ã‰motionnelle** : Les Ã©motions du systÃ¨me limbique influencent l'activation neuronal
- **Adaptation Contextuelle** : L'activation neuronal s'adapte au contexte Ã©motionnel
- **Apprentissage Continu** : Le rÃ©seau s'adapte Ã  travers les interactions

---

## Architecture DÃ©taillÃ©e

### 1. ModeleRNN (modele_rnn.py)

#### Classes

**MultiHeadAttention**
```python
class MultiHeadAttention(nn.Module):
    """MÃ©canisme d'attention multi-tÃªte"""
    
    __init__(hidden_size, num_heads=4)
    forward(query, key, value) -> (output, attention_weights)
```

**ModeleRNN**
```python
class ModeleRNN(nn.Module):
    """LSTM bi-directionnel avec attention et Ã©tat persistant"""
    
    __init__(input_size, hidden_size=256, output_size=512, 
             num_layers=3, num_heads=4, dropout=0.3, 
             bidirectional=True, use_attention=True, device=None)
    
    forward(x, use_persistent_state=False) -> Tensor
    reset_persistent_state() -> None
    get_state_history() -> List[Tuple]
    set_persistent_state(h, c) -> None
```

#### Flux de Traitement

```
Input (batch, seq_len, input_size)
    â†“
[Embedding optionnel si input_size < 512]
    â†“
Multi-Head Attention
    â†“
Bi-directional LSTM (3 couches)
    â†“
Batch Norm 1
    â†“
Dense Layer 1 (GELU + Dropout)
    â†“
Batch Norm 2
    â†“
Dense Layer 2 (GELU + Dropout)
    â†“
Batch Norm 3
    â†“
Output Dense Layer
    â†“
Layer Norm
    â†“
Output (batch, output_size)
```

### 2. TextualCortex (cortex_limbic/textual_cortex.py)

#### Classe Principale

```python
class TextualCortex:
    """Cortex Textuel Autonome - Cerveau Neuronal Textuel de NETY"""
    
    __init__(hidden_size=256, output_size=512, num_layers=3, 
             num_heads=4, dropout=0.3, device=None, 
             emotion_engine=None, memory_manager=None)
```

#### MÃ©thodes ClÃ©s

**process_text_sequence()**
- Traite une sÃ©quence textuelle via le RNN
- Applique la modulation Ã©motionnelle du limbic system
- Maintient l'Ã©tat persistant
- Retourne: (neural_output, metadata)

**_apply_emotional_modulation()**
- Calcule le facteur Ã©motionnel (positif - nÃ©gatif)
- Modifie l'activation neuronal : output * (1 + factor * 0.3)
- Ã‰motions positives augmentent l'activation
- Ã‰motions nÃ©gatives la rÃ©duisent

**_calculate_neural_activation()**
- Calcule le niveau d'activation (0-1)
- Activation = norm_moyenne(output)
- Met Ã  jour les statistiques

**Ã‰tat Neural Persistant**
```python
neural_state = {
    "timestamp": datetime,
    "activation_level": float,  # [0, 1]
    "attention_focus": Optional,
    "emotional_modulation": Dict
}
```

---

## IntÃ©gration au Cerveau (Brain)

### Modifications du Brain.py

1. **Import du TextualCortex**
```python
from nety.cortex_limbic.textual_cortex import TextualCortex
```

2. **Initialisation dans __init__**
```python
self.textual_cortex = TextualCortex(
    hidden_size=256,
    output_size=512,
    num_layers=3,
    num_heads=4,
    dropout=0.3,
    emotion_engine=self.emotion_engine,
    memory_manager=self.memory
)
```

3. **Pipeline de Traitement**
```
Message d'entrÃ©e
    â†“
Analyse d'intention
    â†“
RÃ©cupÃ©ration contextuelle
    â†“
Filtrage limbique
    â†“
[NOUVEAU] Traitement RNN Cortex Textuel âœ¨
    â”œâ”€ Conversion en embedding
    â”œâ”€ Traitement RNN avec attention
    â”œâ”€ Modulation Ã©motionnelle
    â””â”€ Mise Ã  jour Ã©tat persistant
    â†“
GÃ©nÃ©ration de rÃ©ponse
    â†“
Ingestion ML
    â†“
Sortie utilisateur
```

---

## Utilisation

### Installation

```bash
# Assurez-vous que PyTorch est installÃ©
pip install torch

# Les dÃ©pendances sont dÃ©jÃ  dans requirements.txt
```

### Utilisation Simple

```python
from nety.cortex_limbic.textual_cortex import TextualCortex
import torch

# CrÃ©er le cortex
cortex = TextualCortex(hidden_size=256, output_size=512)

# CrÃ©er un embedding (768 dimensions)
embedding = torch.randn(1, 5, 768)  # (batch, seq_len, features)

# Traiter
output, metadata = cortex.process_text_sequence(embedding)

# Afficher les rÃ©sultats
print(f"Activation: {metadata['activation_level']:.3f}")
print(f"Output shape: {output.shape}")
```

### Utilisation avec Ã‰motions

```python
from nety.cortex_limbic.emotion_engine import EmotionEngine

emotion_engine = EmotionEngine()
cortex = TextualCortex(emotion_engine=emotion_engine)

# Traiter avec modulation Ã©motionnelle
emotional_context = {
    "emotions": emotion_engine.emotions
}

output, metadata = cortex.process_text_sequence(
    embedding,
    emotional_context=emotional_context,
    use_persistent_state=True
)
```

### Utilisation du Cerveau Complet

```python
from nety.core.brain import Brain

# Initialiser le cerveau (le cortex est crÃ©Ã© automatiquement)
brain = Brain()

# Traiter un message (le cortex textuel est utilisÃ© automatiquement)
response = brain.think("Bonjour, comment Ã§a va?")

# AccÃ©der aux statistiques neurales
stats = brain.textual_cortex.get_neural_statistics()
print(f"Activation: {stats['current_activation']:.3f}")
```

---

## Statistiques et Monitoring

### Statistiques Disponibles

```python
stats = cortex.get_neural_statistics()

# Contient:
# - total_activations: Nombre total d'activations
# - average_activation: Moyenne des 100 derniÃ¨res activations
# - peak_activation: Pic d'activation observÃ©
# - current_activation: Activation actuelle
# - context_depth: Profondeur de la mÃ©moire d'Ã©tat
# - last_update: Timestamp de la derniÃ¨re mise Ã  jour
```

### Contexte Summary

```python
context = cortex.get_context_summary()

# Contient:
# - window_size: Nombre d'interactions en fenÃªtre
# - neural_state: Ã‰tat neuronal actuel
# - activation_stats: Statistiques complÃ¨tes
# - recent_interactions: 5 derniÃ¨res interactions
```

---

## Performance et Optimisation

### ParamÃ¨tres d'Optimisation

| ParamÃ¨tre | Valeur | Description |
|-----------|--------|-------------|
| hidden_size | 256 | Taille de l'Ã©tat cachÃ© LSTM |
| output_size | 512 | Taille de la reprÃ©sentation de sortie |
| num_layers | 3 | Nombre de couches LSTM |
| num_heads | 4 | Nombre de tÃªtes d'attention |
| dropout | 0.3 | Taux de dropout |
| bidirectional | True | LSTM bi-directionnel |
| use_attention | True | Attention multi-tÃªte |

### MÃ©moire et ComplexitÃ©

**Nombre de ParamÃ¨tres:** ~3.5M paramÃ¨tres

**Utilisation MÃ©moire:**
- ModÃ¨le: ~15 MB
- Ã‰tat persistant (h, c): ~2 MB
- Historique d'Ã©tat (50): ~100 MB
- Contexte glissant: Minimal (~1 MB)

**ComplexitÃ© Computationnelle:**
- Forward pass: O(seq_len Ã— hidden_sizeÂ²)
- Avec attention: +O(seq_lenÂ² Ã— hidden_size)

### Optimisations ImplÃ©mentÃ©es

1. **Batch First LSTM** : Meilleure localitÃ© mÃ©moire
2. **Gradient Checkpointing** : Possible avec torch.utils.checkpoint
3. **Mixed Precision** : Compatible avec torch.amp
4. **State Pruning** : Historique limitÃ© Ã  50 Ã©tats

---

## Tests et DÃ©monstration

### Tests Disponibles

**test_textual_cortex.py**
```bash
python tests/test_textual_cortex.py
```

Tests effectuÃ©s:
1. ModÃ¨le RNN brut
2. Cortex Textuel autonome
3. Modulation Ã©motionnelle
4. Statistiques neurales
5. Persistance d'Ã©tat

### DÃ©monstration Interactive

**Mode AutomatisÃ©:**
```bash
python scripts/demo_rnn_cortex.py --mode demo
```

**Mode Interactif:**
```bash
python scripts/demo_rnn_cortex.py --mode interactive
```

---

## AmÃ©liorations Futures

### Court Terme
- [ ] Sauvegarde/restauration d'Ã©tat complet
- [ ] Quantization pour dÃ©ploiement
- [ ] Export ONNX

### Moyen Terme
- [ ] IntÃ©gration avec transformers
- [ ] Fine-tuning supervisÃ©
- [ ] MÃ©triques d'attention

### Long Terme
- [ ] Apprentissage par renforcement
- [ ] Fusion multi-modale
- [ ] CompÃ©tition de modÃ¨les

---

## Troubleshooting

### Erreur: "CUDA out of memory"
â†’ RÃ©duire `batch_size` ou `hidden_size`

### Activation neuronal = 0
â†’ VÃ©rifier l'embedding d'entrÃ©e (ne doit pas Ãªtre tous zÃ©ros)

### Ã‰tat persistant ne s'accumule pas
â†’ VÃ©rifier `use_persistent_state=True`

### Performance lente
â†’ RÃ©duire `num_layers` ou `seq_length`

---

## RÃ©fÃ©rences

- LSTM: [Hochreiter & Schmidhuber, 1997]
- Attention: [Vaswani et al., 2017]
- Bi-directional RNN: [Schuster & Paliwal, 1997]
- Batch Normalization: [Ioffe & Szegedy, 2015]

---

## Auteur & Historique

- **CrÃ©ation Initiale:** RNN simple (LSTM 2 couches)
- **Modernisation 2026:** Bi-directional + Attention + Modulation Ã©motionnelle
- **IntÃ©gration NETY:** Cortex Textuel Autonome du Cerveau

**Ã‰tat:** âœ… OpÃ©rationnel et IntÃ©grÃ©

---

*Documentation gÃ©nÃ©rÃ©e pour NETY - Cortex Textuel RNN v2.0*
