"""
NETY V2-Maxx - Demo Rapide
===========================

D√©monstration rapide de l'architecture compl√®te sans entra√Ænement.
Montre le flow complet : Input ‚Üí Pipeline ‚Üí Output
"""

import torch
import os
import sys

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from nety.settings import NETYSettings
from nety.models.nety_brain_v2 import NETYBrainV2, NETYBrainConfig
from nety.preprocessing.text_preprocessor import Preprocessor
from nety.postprocessing.text_postprocessor import Postprocessor


def demo_pipeline():
    """D√©montre le pipeline complet"""
    print("=" * 80)
    print("NETY V2-Maxx - D√©monstration Pipeline Complet")
    print("=" * 80)
    
    # 1. Configuration
    print("\n1Ô∏è‚É£  CONFIGURATION")
    print("-" * 80)
    settings = NETYSettings()
    print(f"‚úì Vocabulaire: {settings.model.vocab_size:,} tokens")
    print(f"‚úì Embedding dim: {settings.model.embedding_dim}")
    print(f"‚úì Architecture: Input ‚Üí Cognitive (4 layers) ‚Üí Limbic (6 emotions)")
    print(f"               ‚Üí RNN Encoder (3 Bi-LSTM) ‚Üí RNN Decoder (3 LSTM) ‚Üí Output")
    print(f"‚úì Param√®tres estim√©s: {settings.model.estimate_parameters() / 1e6:.1f}M")
    print(f"‚úì VRAM estim√©e: {settings.model.estimate_vram_usage_gb(settings.training.batch_size):.2f} GB")
    
    # 2. Preprocessing
    print("\n2Ô∏è‚É£  PREPROCESSING")
    print("-" * 80)
    
    # Charger ou cr√©er preprocessor
    if os.path.exists("data/tokenizer/vocab.json"):
        preprocessor = Preprocessor.load("data/tokenizer")
    else:
        print("‚ö†Ô∏è  Tokenizer non trouv√©. Cr√©ation d'un tokenizer de demo...")
        preprocessor = Preprocessor(vocab_size=1000, max_length=256)
        demo_corpus = [
            "Bonjour comment vas-tu",
            "Je vais bien merci",
            "Qu'est-ce que tu fais",
            "Je discute avec toi"
        ]
        preprocessor.fit(demo_corpus, min_frequency=1)
    
    # Test preprocessing
    user_input = "Bonjour, comment vas-tu aujourd'hui ?"
    print(f"‚úì Input: {user_input}")
    
    encoded = preprocessor(user_input)
    print(f"‚úì Tokenization: {len([id for id in encoded['input_ids'] if id != 0])} tokens (+ padding)")
    print(f"‚úì IDs: {encoded['input_ids'][:20]}... (truncated)")
    
    # 3. Mod√®le
    print("\n3Ô∏è‚É£  MOD√àLE NEURONAL")
    print("-" * 80)
    
    config = NETYBrainConfig(
        vocab_size=len(preprocessor.tokenizer.token_to_id),
        embedding_dim=settings.model.embedding_dim,
        max_seq_length=settings.model.max_seq_length,
        cognitive_num_layers=settings.model.cognitive_num_layers,
        rnn_encoder_hidden_dim=settings.model.rnn_encoder_hidden_dim,
        rnn_decoder_hidden_dim=settings.model.rnn_decoder_hidden_dim
    )
    
    model = NETYBrainV2(config)
    model.eval()
    
    print(f"‚úì Mod√®le cr√©√©: {model.count_parameters():,} param√®tres")
    print(f"‚úì Architecture d√©taill√©e:")
    print(f"   - Embedding: {config.vocab_size} √ó {config.embedding_dim}")
    print(f"   - Cognitive Layer: {config.cognitive_num_layers} Transformer layers")
    print(f"   - Limbic System: {config.limbic_num_emotions} emotions")
    print(f"   - RNN Encoder: {config.rnn_encoder_num_layers} Bi-LSTM layers")
    print(f"   - RNN Decoder: {config.rnn_decoder_num_layers} LSTM layers + Attention")
    
    # 4. Forward Pass (Encoding)
    print("\n4Ô∏è‚É£  ENCODING (Input ‚Üí Representations)")
    print("-" * 80)
    
    input_ids = torch.tensor([encoded['input_ids']])
    input_mask = torch.tensor([encoded['attention_mask']])
    
    with torch.no_grad():
        # Encoder
        encoder_outputs, encoder_hidden, emotion_logits = model.encode(
            input_ids, input_mask
        )
    
    print(f"‚úì Encoder outputs shape: {encoder_outputs.shape}")
    print(f"‚úì Emotion logits shape: {emotion_logits.shape}")
    
    # Afficher √©motions pr√©dites
    emotions = ['joie', 'tristesse', 'col√®re', 'peur', 'surprise', 'neutre']
    emotion_probs = torch.softmax(emotion_logits[0], dim=0)
    top_emotion_idx = torch.argmax(emotion_probs).item()
    print(f"‚úì √âmotion dominante: {emotions[top_emotion_idx]} ({emotion_probs[top_emotion_idx]:.2%})")
    
    # 5. G√©n√©ration
    print("\n5Ô∏è‚É£  G√âN√âRATION (Decoding)")
    print("-" * 80)
    
    with torch.no_grad():
        generated_ids = model.generate(
            input_ids=input_ids,
            max_length=50,
            temperature=settings.generation.temperature,
            top_k=settings.generation.top_k,
            top_p=settings.generation.top_p
        )
    
    print(f"‚úì Generated IDs shape: {generated_ids.shape}")
    print(f"‚úì IDs: {generated_ids[0].tolist()[:20]}... (truncated)")
    
    # 6. Postprocessing
    print("\n6Ô∏è‚É£  POSTPROCESSING")
    print("-" * 80)
    
    # D√©tokenization
    raw_text = preprocessor.decode(generated_ids[0].tolist(), skip_special_tokens=True)
    print(f"‚úì Raw output: {raw_text[:100]}...")
    
    # Postprocessing
    postprocessor = Postprocessor()
    cleaned_text = postprocessor(raw_text)
    
    if cleaned_text:
        print(f"‚úì Cleaned output: {cleaned_text[:100]}...")
    else:
        print("‚úì Output rejected by content filter (too short/inappropriate)")
    
    # 7. Pipeline Complet
    print("\n7Ô∏è‚É£  PIPELINE COMPLET R√âSUM√â")
    print("-" * 80)
    print("Input (texte brut)")
    print("  ‚Üì Preprocessing (normalisation, tokenization, encoding)")
    print("Tokens IDs + Attention Mask")
    print("  ‚Üì Embedding Layer")
    print("Token Embeddings (512 dims)")
    print("  ‚Üì Cognitive Layer (4 Transformer Encoder layers)")
    print("Cognitive Representations")
    print("  ‚Üì Limbic System (emotional modulation)")
    print("Modulated Representations + Emotion Prediction")
    print("  ‚Üì RNN Encoder (3 Bi-LSTM layers)")
    print("Encoder Outputs + Hidden State")
    print("  ‚Üì RNN Decoder (3 LSTM layers + Attention)")
    print("Generated Token IDs (autoregressive)")
    print("  ‚Üì Postprocessing (detokenization, formatting, filtering)")
    print("Output (texte nettoy√©)")
    print("-" * 80)
    
    # Stats finales
    print("\nüìä STATISTIQUES FINALES")
    print("-" * 80)
    print(f"‚úì Mod√®le: {model.count_parameters() / 1e6:.1f}M param√®tres")
    print(f"‚úì Vocabulaire: {len(preprocessor.tokenizer.token_to_id):,} tokens")
    print(f"‚úì Input tokens: {sum(encoded['attention_mask'])}")
    print(f"‚úì Output tokens: {(generated_ids[0] != config.pad_token_id).sum().item()}")
    print(f"‚úì √âmotion: {emotions[top_emotion_idx]}")
    
    print("\n" + "=" * 80)
    print("‚úÖ D√âMONSTRATION TERMIN√âE")
    print("=" * 80)
    print("\nüí° Note: Le mod√®le n'est pas entra√Æn√©, donc les sorties sont al√©atoires.")
    print("   Apr√®s entra√Ænement, NETY g√©n√©rera des r√©ponses coh√©rentes et naturelles.")
    print("\nüöÄ Pour entra√Æner: python scripts/train.py")
    print("üó£Ô∏è  Pour chatbot: python scripts/inference.py --mode chat")


if __name__ == "__main__":
    demo_pipeline()
