"""
Module de r√©seau de neurones r√©current (RNN) pour le traitement de texte.

Ce module impl√©mente un LSTM bi-directionnel avanc√© avec :
  - Attention mechanism multi-head
  - Couches LSTM avec dropout
  - Batch normalization
  - Gestion persistante de l'√©tat cach√©

Architecture:
  - Embedding couches
  - Multi-head Attention
  - Bi-directional LSTM empil√©
  - Couches fully-connected denses
  - Sortie avec normalisation

Am√©lior√© et modernis√© pour √™tre le cortex c√©r√©bral textuel autonome de NETY.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional, Tuple, List

class MultiHeadAttention(nn.Module):
    """M√©canisme d'attention multi-t√™te pour le contexte textuel."""
    
    def __init__(self, hidden_size: int, num_heads: int = 4) -> None:
        super(MultiHeadAttention, self).__init__()
        assert hidden_size % num_heads == 0, "hidden_size doit √™tre divisible par num_heads"
        
        self.hidden_size = hidden_size
        self.num_heads = num_heads
        self.head_dim = hidden_size // num_heads
        
        self.query = nn.Linear(hidden_size, hidden_size)
        self.key = nn.Linear(hidden_size, hidden_size)
        self.value = nn.Linear(hidden_size, hidden_size)
        self.fc_out = nn.Linear(hidden_size, hidden_size)
        
    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """Calcule l'attention multi-t√™te."""
        batch_size = query.shape[0]
        
        # Projection lin√©aire
        Q = self.query(query)
        K = self.key(key)
        V = self.value(value)
        
        # Reshape pour les t√™tes multiples
        Q = Q.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)
        K = K.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)
        V = V.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)
        
        # Scores d'attention
        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)
        attention = F.softmax(scores, dim=-1)
        
        # Valeurs pond√©r√©es
        context = torch.matmul(attention, V)
        
        # Combiner les t√™tes
        context = context.transpose(1, 2).contiguous()
        context = context.view(batch_size, -1, self.hidden_size)
        output = self.fc_out(context)
        
        return output, attention


class ModeleRNN(nn.Module):
    """
    R√©seau de neurones r√©current bi-directionnel avanc√© avec attention.
    
    Architecture:
        - Embedding optionnel
        - Multi-head Attention
        - Bi-directional LSTM empil√© avec dropout
        - Batch normalization inter-couches
        - Couches enti√®rement connect√©es denses
        - Gestion persistante d'√©tat cach√©
    
    Args:
        input_size: Dimension des features d'entr√©e (ou vocab_size si embedding)
        hidden_size: Dimension de l'√©tat cach√© du LSTM (d√©faut: 256)
        output_size: Dimension de la sortie
        num_layers: Nombre de couches LSTM (d√©faut: 3)
        num_heads: Nombre de t√™tes d'attention (d√©faut: 4)
        dropout: Taux de dropout (d√©faut: 0.3)
        bidirectional: LSTM bi-directionnel (d√©faut: True)
        use_attention: Activer le m√©canisme d'attention (d√©faut: True)
        device: Device PyTorch (cpu ou cuda, d√©faut: auto)
    """
    
    def __init__(
        self, 
        input_size: int, 
        hidden_size: int = 256,
        output_size: int = 512,
        num_layers: int = 3,
        num_heads: int = 4,
        dropout: float = 0.3,
        bidirectional: bool = True,
        use_attention: bool = True,
        device: Optional[str] = None
    ) -> None:
        super(ModeleRNN, self).__init__()
        
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size
        self.num_layers = num_layers
        self.dropout = dropout
        self.bidirectional = bidirectional
        self.use_attention = use_attention
        
        # D√©terminer le device
        if device is None:
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        else:
            self.device = torch.device(device)
        
        # √âtat cach√© persistant pour interactions autonomes
        self.persistent_h = None
        self.persistent_c = None
        self.state_history: List[Tuple] = []
        
        # Couche d'embedding (optionnelle)
        # Peut √™tre utilis√©e si input_size < 512
        if input_size < 512:
            self.embedding = nn.Embedding(input_size, 128)
            lstm_input_size = 128
        else:
            self.embedding = None
            lstm_input_size = input_size
        
        # Attention multi-t√™te
        if self.use_attention:
            self.attention = MultiHeadAttention(lstm_input_size, num_heads)
        
        # Couches LSTM bi-directionnelles
        self.lstm = nn.LSTM(
            lstm_input_size,
            hidden_size,
            num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0,
            bidirectional=bidirectional
        )
        
        # Batch normalization apr√®s LSTM
        lstm_output_size = hidden_size * (2 if bidirectional else 1)
        self.batch_norm1 = nn.BatchNorm1d(lstm_output_size)
        
        # Couches fully-connected denses
        self.fc1 = nn.Linear(lstm_output_size, hidden_size)
        self.batch_norm2 = nn.BatchNorm1d(hidden_size)
        
        self.fc2 = nn.Linear(hidden_size, hidden_size // 2)
        self.batch_norm3 = nn.BatchNorm1d(hidden_size // 2)
        
        self.fc3 = nn.Linear(hidden_size // 2, output_size)
        
        # Couches d'activation
        self.dropout_layer = nn.Dropout(dropout)
        self.activation = nn.GELU()
        
        # Layer normalization finale
        self.layer_norm = nn.LayerNorm(output_size)
        
    def forward(
        self, 
        x: torch.Tensor, 
        use_persistent_state: bool = False
    ) -> torch.Tensor:
        """
        Propagation avant √† travers le r√©seau.
        
        Args:
            x: Tensor d'entr√©e de forme (batch_size, sequence_length, input_size)
               ou (batch_size, sequence_length) si embedding
            use_persistent_state: Utiliser l'√©tat cach√© persistant (d√©faut: False)
            
        Returns:
            Tensor de sortie de forme (batch_size, output_size)
        """
        # D√©terminer le device r√©el du mod√®le (en se basant sur le premier param√®tre)
        device = next(self.parameters()).device
        
        # Assurer que l'entr√©e est sur le bon device
        x = x.to(device)
        
        # Embedding optionnel
        if self.embedding is not None:
            x = self.embedding(x)
        
        # Attention optionnelle
        if self.use_attention:
            x, _ = self.attention(x, x, x)
        
        # √âtats cach√©s initiaux ou persistants
        if use_persistent_state and self.persistent_h is not None and self.persistent_c is not None:
            h0 = self.persistent_h.to(device)
            c0 = self.persistent_c.to(device)
        else:
            batch_size = x.size(0)
            lstm_hidden_size = self.hidden_size * (2 if self.bidirectional else 1)
            h0 = torch.zeros(self.num_layers * (2 if self.bidirectional else 1), batch_size, self.hidden_size).to(device)
            c0 = torch.zeros(self.num_layers * (2 if self.bidirectional else 1), batch_size, self.hidden_size).to(device)
        
        # Propagation LSTM
        lstm_out, (hn, cn) = self.lstm(x, (h0, c0))
        
        # Sauvegarder l'√©tat persistant
        self.persistent_h = hn.detach()
        self.persistent_c = cn.detach()
        self.state_history.append((hn.detach(), cn.detach()))
        
        # Limiter l'historique d'√©tat
        if len(self.state_history) > 50:
            self.state_history = self.state_history[-50:]
        
        # Prendre la sortie de la derni√®re √©tape temporelle
        out = lstm_out[:, -1, :]
        
        # Batch norm
        out = self.batch_norm1(out)
        
        # Couches fully-connected avec activations
        out = self.activation(self.fc1(out))
        out = self.dropout_layer(out)
        out = self.batch_norm2(out)
        
        out = self.activation(self.fc2(out))
        out = self.dropout_layer(out)
        out = self.batch_norm3(out)
        
        # Sortie finale
        out = self.fc3(out)
        out = self.layer_norm(out)
        
        return out
    
    def reset_persistent_state(self) -> None:
        """R√©initialiser l'√©tat cach√© persistant."""
        self.persistent_h = None
        self.persistent_c = None
        self.state_history.clear()
    
    def get_state_history(self) -> List[Tuple]:
        """R√©cup√©rer l'historique des √©tats cach√©s."""
        return self.state_history
    
    def set_persistent_state(self, h: torch.Tensor, c: torch.Tensor) -> None:
        """D√©finir manuellement l'√©tat cach√© persistant."""
        self.persistent_h = h
        self.persistent_c = c


if __name__ == "__main__":
    # Exemple de cr√©ation et utilisation du mod√®le
    print("üß† Initialisation du Cortex Textuel RNN de NETY...\n")
    
    modele_rnn = ModeleRNN(
        input_size=768,  # Embedding dimension (ex: FastText, Word2Vec)
        hidden_size=256,
        output_size=512,
        num_layers=3,
        num_heads=4,
        dropout=0.3,
        bidirectional=True,
        use_attention=True
    )
    
    print(modele_rnn)
    print(f"\nüìä Total de param√®tres: {sum(p.numel() for p in modele_rnn.parameters()):,}")
    
    # Test avec une entr√©e al√©atoire
    batch_size, seq_length = 2, 10
    test_input = torch.randn(batch_size, seq_length, 768)
    output = modele_rnn(test_input)
    print(f"‚úÖ Sortie du mod√®le: {output.shape}")