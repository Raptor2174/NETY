"""
Module Brain - Cerveau central de NETY
"""
# nety/core/brain.py
from typing import Optional
import torch

# Imports locaux
from nety.cortex_limbic.emotion_engine import EmotionEngine
from nety.cortex_limbic.limbic_filter import LimbicFilter
from nety.cortex_limbic.memory_manager import MemoryManager
# Import lazy pour TextualCortex (√©vite les imports circulaires)
from nety.knowledge_base.knowledge_manager import KnowledgeManager
from nety.core.intent_analyzer import IntentAnalyzer
from nety.core.response_generator import ResponseGenerator
from nety.core.llm_config import LLMConfig
from nety.modules.machinelearning.ml_engine import MLEngine

# Import lazy de TextualCortex
TextualCortex = None



class Brain:
    """Le cerveau principal de NETYOrchestre tous les modules et g√®re l'interaction avec le syst√®me"""
    
    def __init__(self, model_type: Optional[str] = None):
        # Initialisation des modules
        self.limbic_filter = LimbicFilter()
        self.memory = MemoryManager()
        self.knowledge = KnowledgeManager()
        self.intent_analyzer = IntentAnalyzer()
        self.ml_engine = MLEngine()
        self.emotion_engine = EmotionEngine()
        
        # ‚ú® Initialiser le Cortex Textuel RNN - Cerveau Neuronal Textuel Autonome (lazy)
        print("üß† Initialisation du Cortex Textuel (RNN bi-directionnel)...")
        try:
            from nety.cortex_limbic.textual_cortex import TextualCortex as _TextualCortex
            self.textual_cortex = _TextualCortex(
                hidden_size=256,
                output_size=512,
                num_layers=3,
                num_heads=4,
                dropout=0.3,
                emotion_engine=self.emotion_engine,
                memory_manager=self.memory
            )
        except ImportError as e:
            print(f"‚ö†Ô∏è Erreur d'importation du Cortex Textuel: {e}")
            self.textual_cortex = None
        
        # D√©terminer le mod√®le √† utiliser
        if model_type is None:
            model_type = LLMConfig().CURRENT_MODEL
        
        # Initialiser avec le mod√®le choisi
        print(f"üß† Initialisation du cerveau NETY avec {model_type.upper()}...")
        self.response_generator = ResponseGenerator(model_type=model_type)
        
        # Afficher les infos
        print(f"‚úÖ Mod√®le charg√©: {model_type.upper()}")
        print(f"üìä Contexte max: 8192 tokens")
        
        # Historique des interactions pour get_context()
        self.context_history = []
        
        # √âtat des modules
        self.modules_status = {
            "cortex_textuel": "actif",
            "cortex_limbic": "actif",
            "memory": "actif",
            "knowledge_base": "actif",
            "intent_analyzer": "actif",
            "ml_engine": "actif"
        }
        
        # Dictionnaire des modules pour compatibilit√©
        self.modules = {}
        self.context = {}
        self.state = "active"
    
    def think(self, message: str) -> str:
        """M√©thode principale pour traiter un message"""
        
        # ‚úÖ NETTOYER LE MESSAGE AVANT STOCKAGE
        # Retirer les pr√©fixes m√™me ici (au cas o√π)
        cleaned_message = message
        for prefix in ["CHAT: ", "PROMPT: ", "CHAT:", "PROMPT:"]:
            if cleaned_message.startswith(prefix):
                cleaned_message = cleaned_message[len(prefix):].strip()
                break
        
        # Stocker l'entr√©e NETTOY√âE
        interaction = {"input": cleaned_message}  # ‚úÖ Version propre
        
        # Traiter le message via le pipeline complet
        response = self.process_message(cleaned_message)  # ‚úÖ Ici aussi
        
        # Stocker la sortie
        interaction["output"] = response
        self.context_history.append(interaction)
        
        # Limiter l'historique √† 100 interactions
        if len(self.context_history) > 100:
            self.context_history = self.context_history[-100:]
        
        return self.ml_engine.generate_response(message)
    
    def _identify_user(self) -> Optional[str]:
        """Identifie l'utilisateur bas√© sur les key_info.jsonl"""
        try:
            key_infos = self.ml_engine.load_key_info()
            if key_infos:
                # Chercher la derni√®re identit√© enregistr√©e
                for key_info in reversed(key_infos):
                    if key_info.get("type") == "user_identity":
                        return key_info.get("user_id")
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur lors de l'identification utilisateur: {e}")
        return None

    def retrieve_context(self, message: str, intent: dict) -> dict:
        """R√©cup√®re le contexte bas√© sur le message et l'intention"""
        
        # R√©cup√©rer les connaissances pertinentes
        knowledge_data = {}
        if hasattr(self.knowledge, 'get_knowledge'):
            knowledge_data = self.knowledge.get_knowledge(intent.get('type', 'general'))
        
        # ‚úÖ EXTRAIRE LES INFORMATIONS CL√âS DE L'HISTORIQUE
        user_name = None
        for interaction in reversed(self.context_history[-10:]):
            user_msg = interaction.get('input', '').lower()
            # D√©tecter "je m'appelle X" ou "je suis X"
            if "je m'appel" in user_msg or "je suis" in user_msg:
                # Extraire le nom (simpliste)
                words = user_msg.split()
                try:
                    if "m'appel" in user_msg:
                        idx = words.index("m'appel") if "m'appel" in words else words.index("m'appelle")
                        user_name = words[idx + 1].strip('.,!?')
                    elif "je suis" in user_msg:
                        idx = words.index("suis")
                        user_name = words[idx + 1].strip('.,!?')
                except:
                    pass
        
        ml_profile = self.ml_engine.get_user_profile()
        if not user_name:
            user_name = ml_profile.get("name")

        # ‚ú® UTILISER LA NOUVELLE M√âTHODE DE R√âCUP√âRATION DE M√âMOIRES AVEC CONTEXTE
        personal_memories = self.ml_engine.get_relevant_memories(message, limit=10)
        
        # ‚úÖ CHARGER LES KEY_INFO (identit√©, r√¥les, etc.)
        key_infos = self.ml_engine.load_key_info()
        
        # ‚úÖ D√âTECTER L'UTILISATEUR POUR LE USER_ID
        user_id = self._identify_user()

        context = {
            "message": message,
            "intent": intent,
            "history": self.context_history[-5:],
            "knowledge": knowledge_data,
            "user_name": user_name,  # ‚úÖ Info cl√© extraite
            "personal_memory": personal_memories,  # ‚ú® M√©moires am√©lior√©es avec labels et corr√©lations
            "user_profile": ml_profile,
            "key_infos": key_infos,  # ‚úÖ Infos cl√©s (identit√©, r√¥les)
            "user_id": user_id,  # ‚úÖ ID utilisateur d√©tect√©
            "memory_context": {  # ‚ú® Contexte de m√©moire enrichi
                "recent_labels": self._extract_memory_labels(personal_memories),
                "memory_sentiment": self._extract_memory_sentiment(personal_memories),
            }
        }
        return context
    
    def _extract_memory_labels(self, memories: list) -> list:
        """Extrait les labels des souvenirs pour enrichir le contexte"""
        labels = set()
        for memory in memories:
            if isinstance(memory, dict) and "labels" in memory:
                labels.update(memory.get("labels", []))
        return list(labels)
    
    def _extract_memory_sentiment(self, memories: list) -> str:
        """D√©termine le sentiment global des souvenirs r√©cents"""
        sentiments = []
        for memory in memories:
            if isinstance(memory, dict) and "meta" in memory:
                sent = memory.get("meta", {}).get("sentiment", "neutral")
                sentiments.append(sent)
        
        if not sentiments:
            return "neutral"
        
        pos = sentiments.count("positive")
        neg = sentiments.count("negative")
        
        if pos > neg:
            return "positive"
        elif neg > pos:
            return "negative"
        else:
            return "neutral"
    
    def process_message(self, message: str) -> str:
        """Pipeline complet de traitement"""
        
        # [1] Analyse d'intention
        intent = self.intent_analyzer.analyze(message)
        
        # [2] R√©cup√©ration contextuelle
        context = self.retrieve_context(message, intent)
        
        # [3] Filtrage limbique avanc√© ‚ú®
        personality_filter = self.limbic_filter.apply_filter(context)
        
        # [3.5] ‚ú® TRAITEMENT RNN DU CORTEX TEXTUEL (Nouveau!)
        # Traiter le message via le cortex neuronal textuel autonome
        if self.textual_cortex is not None:
            try:
                # Convertir le message en embeddings pour le RNN
                message_embedding = self._get_message_embedding(message)
                if message_embedding is not None:
                    # Traiter via le cortex textuel avec modulation √©motionnelle
                    neural_output, neural_metadata = self.textual_cortex.process_text_sequence(
                        message_embedding,
                        emotional_context={
                            "emotions": self.emotion_engine.emotions
                        },
                        use_persistent_state=True
                    )
                    
                    # Ajouter l'activation neuronal au contexte
                    context["neural_activation"] = neural_metadata["activation_level"]
                    context["neural_output"] = neural_output.detach().cpu() if isinstance(neural_output, torch.Tensor) else neural_output
                    
                    # Enregistrer dans la fen√™tre contextuelle du cortex
                    self.textual_cortex.add_to_context_window({
                        "input": message,
                        "timestamp": neural_metadata["timestamp"],
                        "activation": neural_metadata["activation_level"]
                    })
            except Exception as e:
                print(f"‚ö†Ô∏è Cortex textuel processing: {e}")
        
        # [4] G√©n√©ration de r√©ponse
        response = self.response_generator.generate(
            message, context, personality_filter
        )

        # [4.5] Ingestion ML (m√©moire personnelle) avec user_id d√©tect√© et labels
        try:
            user_id = context.get("user_id")
            if user_id:
                print(f"üë§ Utilisateur identifi√©: {user_id}")
            
            # ‚ú® Utiliser la nouvelle m√©thode avec labels et corr√©lations
            memory_entry = self.ml_engine.assign_memory_labels(message, user_id=user_id)
            
            # Enregistrer dans la m√©moire locale avec labels
            if self.memory:
                memory_id = self.memory.add_memory(
                    message[:100],  # R√©sum√© court
                    labels=memory_entry.get("labels", ["other"]),
                    metadata={
                        "user_id": user_id,
                        "categories": memory_entry.get("categories", []),
                        "sentiment": memory_entry.get("meta", {}).get("sentiment", "neutral"),
                        "keywords": memory_entry.get("keywords", [])[:5]
                    }
                )
            
            stats = self.ml_engine.get_stats()
            if stats.get("total_entries", 0) % 20 == 0:
                self.ml_engine.train_from_memory()
        except Exception as exc:
            print(f"‚ö†Ô∏è ML Engine ingestion error: {exc}")
        
        # [5] Enregistrement de l'interaction pour apprentissage ‚ú®
        user_sentiment = self._analyze_user_sentiment(message)
        interaction_data = {
            "message": message,
            "response": response,
            "user_id": context.get("user_id"),
            "emotional_state": personality_filter.get("emotional_state"),
            "user_sentiment": user_sentiment,
            "neural_activation": context.get("neural_activation")
        }
        # Store interaction in memory instead
        self.memory.add_memory(f"Interaction: {message[:50]} -> {response[:50]}")
        
        return response
    
    def _analyze_user_sentiment(self, message: str) -> str:
        """Analyse le sentiment de l'utilisateur √† partir du message"""
        positive_words = ["merci", "super", "g√©nial", "content", "heureux", "aime", "formidable", "excellent"]
        negative_words = ["triste", "nul", "mauvais", "d√©√ßu", "horrible", "d√©teste", "frustr√©"]
        
        message_lower = message.lower()
        
        # Compter les mots positifs et n√©gatifs
        positive_count = sum(1 for word in positive_words if word in message_lower)
        negative_count = sum(1 for word in negative_words if word in message_lower)
        
        if positive_count > negative_count:
            return "positif"
        elif negative_count > positive_count:
            return "n√©gatif"
        else:
            return "neutre"
    
    def _get_message_embedding(self, message: str) -> Optional[torch.Tensor]:
        """
        Convertit un message en embeddings pour le cortex textuel RNN.
        Utilise une dimension de 768 (standard pour les embeddings modernes).
        """
        try:
            import numpy as np
            
            # Pour la d√©mo, cr√©er un embedding bas√© sur le hash du message
            # En production, utiliser un vrai mod√®le d'embedding (FastText, BERT, etc.)
            hash_val = hash(message)
            np.random.seed(abs(hash_val) % (2**31))
            
            # Cr√©er un embedding synth√©tique (768 dimensions)
            embedding = np.random.randn(1, 1, 768).astype(np.float32)
            
            # Normaliser
            embedding = embedding / (np.linalg.norm(embedding) + 1e-8)
            
            return torch.from_numpy(embedding)
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur lors de la cr√©ation d'embedding: {e}")
            return None
    
    def get_modules_status(self):
        """Retourne l'√©tat de tous les modules"""
        return self.modules_status.copy()
    
    def register_module(self, name, module):
        """Enregistre un nouveau module"""
        self.modules[name] = module
    
    def shutdown(self):
        """Arr√™te proprement le Brain"""
        self.state = "stopped"
        print("üß† Brain arr√™t√©")
    
    # ===== M√©thodes suppl√©mentaires pour compatibilit√© =====
    
    def process(self, data):
        """Alias pour think()"""
        return self.think(data)
    
    def add_to_memory(self, input_data, output_data):
        """Ajoute une interaction √† la m√©moire"""
        summary = f"Input: {str(input_data)[:50]} | Output: {str(output_data)[:50]}"
        self.memory.add_memory(summary)
    
    def get_memory(self):
        """R√©cup√®re la m√©moire"""
        return self.memory
    
    def clear(self):
        """Vide le Brain (m√©moire et contexte)"""
        self.clear_memory()
        self.context_history = []

    def clear_all(self):
        """Vide compl√®tement le Brain"""
        self.clear_memory()
        self.context_history = []
        # R√©initialiser KnowledgeManager en r√©-instanciant l'objet
        self.knowledge = KnowledgeManager()

    def reset(self):
        """R√©initialise le Brain"""
        self.clear_memory()
        self.context_history = []
        self.state = "active"

    def clear_memory(self):
        """Vide la m√©moire"""
        # R√©initialiser MemoryManager en r√©-instanciant l'objet
        self.memory = MemoryManager()
    
    def set_context(self, key, value):
        """D√©finit un √©l√©ment de contexte"""
        self.context[key] = value
    
    def get_context_value(self, key=None):
        """R√©cup√®re le contexte ou un √©l√©ment"""
        if key is None:
            return self.context
        return self.context.get(key)
    
    def get_module(self, name):
        """R√©cup√®re un module sp√©cifique"""
        return self.modules.get(name)
    
    def activate_module(self, name):
        """Active un module"""
        if name in self.modules:
            module = self.modules[name]
            if hasattr(module, 'activate'):
                module.activate()
            return True
        return False
    
    def deactivate_module(self, name):
        """D√©sactive un module"""
        if name in self.modules:
            module = self.modules[name]
            if hasattr(module, 'deactivate'):
                module.deactivate()
            return True
        return False
    
    def get_state(self):
        """Retourne l'√©tat actuel du Brain"""
        return self.state
    
    def set_state(self, new_state):
        """D√©finit un nouvel √©tat"""
        self.state = new_state
    
    def update_memory(self, message: str, response: str):
        """Met √† jour la m√©moire"""
        summary = f"User: {message[:50]}... | Response: {response[:50]}..."
        self.memory.add_memory(summary)
    
    def get_context(self) -> list:
        """
        Retourne l'historique des interactions (contexte)
        Utilis√© pour les tests et le debugging
        
        Returns:
            List of interactions with {input, output} keys
            Liste des interactions {input, output}
        """
        return self.context_history.copy()


# Alias pour compatibilit√©
NETYBrain = Brain
