"""
S√©lecteur de mod√®le interactif pour NETY - Mod√®les locaux uniquement
"""
import os
from typing import Literal

ModelChoice = Literal["mistral", "bloomz"]

class ModelSelector:
    """Gestionnaire de s√©lection de mod√®le"""
    
    def __init__(self):
        from nety.core.llm_config import LLMConfig
        self.config = LLMConfig()
        
        self.available_models = {
            "1": {
                "name": "mistral",
                "display": "Mistral-7B (Local GPU - Puissant mais gourmand)",
                "backend": "local",
                "requires_gpu": True,
                "ram_gb": 8,
                "cost": "Gratuit (utilise ton mat√©riel)",
            },
            "2": {
                "name": "bloomz",
                "display": "BLOOMZ-560M (Local CPU - L√©ger)",
                "backend": "local",
                "requires_gpu": False,
                "ram_gb": 2,
                "cost": "Gratuit (utilise ton mat√©riel)",
            },
            # Future models can be added here

        }
    
    def display_menu(self) -> None:
        """Affiche le menu avec infos de co√ªt"""
        print("\n" + "=" * 70)
        print("ü§ñ S√âLECTION DU MOD√àLE D'IA POUR NETY")
        print("=" * 70)
        print()
        
        for key, model in self.available_models.items():
            print(f"{key}. {model['display']}")
            print(f"   Backend: {model['backend']}")
            print(f"   RAM requise: {model['ram_gb']} GB")
            print(f"   üí∞ Co√ªt: {model['cost']}")
            if model['requires_gpu']:
                print("   ‚ö†Ô∏è N√©cessite un GPU avec 4+ GB VRAM")
            print()
    
    def get_user_choice(self) -> ModelChoice:
        """Demande √† l'utilisateur de choisir"""
        self.display_menu()
        
        while True:
            choice = input("üëâ Choisis ton mod√®le (1 ou 2): ").strip()
            
            if choice in self.available_models:
                selected = self.available_models[choice]
                
                print(f"‚úÖ Mod√®le s√©lectionn√©: {selected['display']}")
                print()
                return selected["name"]
            else:
                print("‚ùå Choix invalide. Entre 1 ou 2.")
    
    def auto_select(self, prefer_local: bool = True) -> ModelChoice:
        """S√©lection automatique"""
        import torch
        
        if torch.cuda.is_available():
            print("ü§ñ Auto-s√©lection: Mistral-7B (GPU d√©tect√©)")
            return "mistral"
        
        print("ü§ñ Auto-s√©lection: BLOOMZ (CPU uniquement)")
        return "bloomz"


def select_model(interactive: bool = True) -> ModelChoice:
    """
    S√©lectionne un mod√®le
    
    Args:
        interactive: Menu interactif ou auto
    
    Returns:
        Nom du mod√®le choisi
    """
    selector = ModelSelector()
    
    if interactive:
        return selector.get_user_choice()
    else:
        return selector.auto_select()