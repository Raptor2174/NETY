# nety/core/rnn_response_generator.py

"""
G√©n√©rateur de r√©ponses bas√© sur le RNN local (TextualCortex)
Utilise le mod√®le RNN bi-directionnel entra√Æn√© pour g√©n√©rer des r√©ponses
"""

import torch
import json
import os
from typing import Dict, Optional, List
import numpy as np


class RNNResponseGenerator:
    """
    G√©n√©rateur de r√©ponses bas√© sur le RNN local
    Utilise le TextualCortex et le vocabulaire entra√Æn√©
    """
    
    def __init__(self, data_dir: str = "data/processed/ml_engine"):
        self.data_dir = data_dir
        self.device = "cuda" if torch.cuda.is_available() else "cpu"

        # Statistiques d'activation
        self.activation_stats = {
            "total_activations": 0,
            "average_activation": 0.0,
            "peak_activation": 0.0,
            "activation_history": []
        }
        
        # Charger le vocabulaire
        self.vocab = self._load_vocab()
        self.word_to_idx = self.vocab.get("word_to_idx", {})
        self.idx_to_word = self.vocab.get("idx_to_word", {})
        self.vocab_size = len(self.word_to_idx)
        
        # Initialiser le RNN
        from nety.cortex_limbic.textual_cortex import TextualCortex
        
        self.textual_cortex = TextualCortex(
            hidden_size=256,
            output_size=512,
            num_layers=3,
            num_heads=4,
            dropout=0.2,  # Moins de dropout pour l'inf√©rence
            device=self.device
        )
        
        # Charger le mod√®le entra√Æn√© si disponible
        self._load_trained_model()
        
        # üî• NOUVEAU : Architecture Hybride RNN-Transformer
        # RNN Encoder (existant via TextualCortex) + Transformer Decoder
        from nety.modules.text.transformer_decoder import HybridRNNTransformer
        
        self.hybrid_model = HybridRNNTransformer(
            vocab_size=max(self.vocab_size, 1000),  # Au moins 1000 pour un vocabulaire minimal
            rnn_hidden_size=256,
            rnn_num_layers=3,
            rnn_num_heads=4,
            decoder_d_model=512,
            decoder_nhead=8,
            decoder_num_layers=6,
            decoder_dim_feedforward=2048,
            dropout=0.1,
            device=self.device
        ).to(self.device)
        
        # Charger le mod√®le entra√Æn√© si disponible
        hybrid_model_path = os.path.join(self.data_dir, "hybrid_model.pt")
        if os.path.exists(hybrid_model_path):
            try:
                state_dict = torch.load(hybrid_model_path, map_location=self.device)
                self.hybrid_model.load_state_dict(state_dict)
                print("‚úÖ Mod√®le hybride RNN-Transformer entra√Æn√© charg√©")
            except Exception as e:
                print(f"‚ö†Ô∏è Impossible de charger le mod√®le hybride: {e}")
        else:
            print("‚ö†Ô∏è Aucun mod√®le hybride entra√Æn√© trouv√©")
        
        print(f"üß† RNN Response Generator initialis√©")
        print(f"   ‚îú‚îÄ Vocabulaire: {self.vocab_size} mots")
        print(f"   ‚îú‚îÄ Device: {self.device}")
        print(f"   ‚îú‚îÄ Architecture: RNN Encoder ‚Üí Transformer Decoder (6L)")
        print(f"   ‚îú‚îÄ Params: {self.hybrid_model.num_params:,}")
        print(f"   ‚îî‚îÄ Mode: G√©n√©ration autoregressive")
    
    def _load_vocab(self) -> Dict:
        """Charge le vocabulaire depuis vocab.json"""
        vocab_path = os.path.join(self.data_dir, "vocab.json")
        if os.path.exists(vocab_path):
            with open(vocab_path, "r", encoding="utf-8") as f:
                return json.load(f)
        return {"word_to_idx": {"<pad>": 0, "<unk>": 1}, "idx_to_word": {"0": "<pad>", "1": "<unk>"}}
    
    def _load_trained_model(self) -> None:
        """Charge le mod√®le RNN entra√Æn√©"""
        model_path = os.path.join(self.data_dir, "model.pt")
        if os.path.exists(model_path):
            try:
                # Le mod√®le est charg√© par le TextualCortex automatiquement
                print("‚úÖ Mod√®le RNN entra√Æn√© charg√©")
            except Exception as e:
                print(f"‚ö†Ô∏è Impossible de charger le mod√®le entra√Æn√©: {e}")
        else:
            print("‚ö†Ô∏è Aucun mod√®le entra√Æn√© trouv√© (utilisation du mod√®le initial)")
    
    def generate(
        self, 
        message: str, 
        context: Optional[Dict] = None,
        limbic_filter: Optional[Dict] = None,
        max_length: int = 50,
        temperature: float = 0.8
    ) -> str:
        """
        G√©n√®re une r√©ponse avec l'architecture hybride RNN-Transformer
        
        Args:
            message: Message de l'utilisateur
            context: Contexte (m√©moire, historique)
            limbic_filter: Filtres √©motionnels
            max_length: Longueur max de la r√©ponse
            temperature: Contr√¥le la cr√©ativit√© (0.5=conservateur, 1.0=cr√©atif)
        """
        
        # [1] Encoder le message en tokens
        input_sequence = self._encode_message(message)
        input_tokens = self._message_to_tokens(message)
        
        # [2] Obtenir le contexte √©motionnel
        emotional_context = self._extract_emotional_context(limbic_filter)
        
        # [3] Traiter via le cortex textuel (RNN Encoder)
        try:
            neural_output, metadata = self.textual_cortex.process_text_sequence(
                input_sequence,
                emotional_context=emotional_context,
                use_persistent_state=True
            )
            
            # [4] G√©n√©rer la r√©ponse avec Transformer Decoder
            response = self._generate_response(
                neural_output, 
                context=context,
                max_length=max_length,
                temperature=temperature,
                input_tokens=input_tokens
            )
            
            # [5] Post-traitement
            response = self._post_process(response)
            
            # Statistiques
            activation = metadata.get("activation_level", 0.0)
            print(f"üß† RNN activation: {activation:.3f}")
            
            return response
            
        except Exception as e:
            print(f"‚ùå Erreur RNN: {e}")
            import traceback
            traceback.print_exc()
            return self._fallback_response(message, context)
    
    def _message_to_tokens(self, message: str) -> torch.Tensor:
        """
        Convertit un message en tokens (indices) pour le transformer
        
        Args:
            message: Message texte
        
        Returns:
            Tensor de tokens (1, seq_len)
        """
        words = message.lower().split()
        
        # Convertir en indices
        indices = []
        for word in words:
            idx = self.word_to_idx.get(word, self.word_to_idx.get("<unk>", 1))
            indices.append(idx)
        
        # Limiter la longueur
        if len(indices) > 50:
            indices = indices[:50]
        
        # Si vide, utiliser <unk>
        if not indices:
            indices = [1]
        
        # Cr√©er le tensor (1, seq_len)
        return torch.LongTensor(indices).unsqueeze(0).to(self.device)
    
    def _encode_message(self, message: str) -> torch.Tensor:
        """
        Encode le message en tensor pour le RNN
        Utilise le vocabulaire entra√Æn√©
        """
        words = message.lower().split()
        
        # Convertir en indices
        indices = []
        for word in words:
            idx = self.word_to_idx.get(word, self.word_to_idx.get("<unk>", 1))
            indices.append(idx)
        
        # Limiter la longueur
        if len(indices) > 50:
            indices = indices[:50]
        
        # Cr√©er un embedding simple (moyenne de one-hot encodings)
        # En production, utiliser un vrai mod√®le d'embedding (Word2Vec, FastText, etc.)
        embedding_dim = 768
        
        # Embedding basique : hash des mots
        embeddings = []
        for idx in indices:
            # Cr√©er un embedding pseudo-al√©atoire mais d√©terministe
            np.random.seed(idx)
            emb = np.random.randn(embedding_dim).astype(np.float32)
            emb = emb / (np.linalg.norm(emb) + 1e-8)
            embeddings.append(emb)
        
        if not embeddings:
            embeddings = [np.zeros(embedding_dim, dtype=np.float32)]
        
        # Shape: (batch=1, seq_len, embedding_dim)
        embeddings_array = np.array(embeddings).reshape(1, len(embeddings), embedding_dim)
        
        return torch.from_numpy(embeddings_array).to(self.device)
    
    def _extract_emotional_context(self, limbic_filter: Optional[Dict]) -> Dict:
        """Extrait le contexte √©motionnel du filtre limbique"""
        if not limbic_filter:
            return {"emotions": {}}
        
        emotional_state = limbic_filter.get("emotional_state", {})
        all_emotions = emotional_state.get("all_emotions", {})
        
        return {"emotions": all_emotions}
    
    def _decode_tokens(
        self, 
        src_tokens: torch.Tensor,
        max_length: int = 50,
        temperature: float = 0.8
    ) -> str:
        """
        D√©code avec l'architecture hybride RNN-Transformer
        
        Args:
            src_tokens: Tokens d'entr√©e (batch, seq_len) pour encoder avec le RNN
            max_length: Nombre maximum de tokens √† g√©n√©rer
            temperature: Contr√¥le la cr√©ativit√© (0.5=conservateur, 1.5=cr√©atif)
            
        Returns:
            Texte g√©n√©r√© token par token avec le transformer decoder
        """
        
        # V√©rifier que le vocabulaire est disponible
        if self.vocab_size < 2:
            return ""  # Pas de vocabulaire, pas de d√©codage possible
        
        # Tokens sp√©ciaux
        start_token = self.word_to_idx.get("<sos>", 1)
        end_token = self.word_to_idx.get("<eos>", 2)
        
        try:
            # G√©n√©rer avec l'architecture hybride RNN-Transformer
            self.hybrid_model.eval()  # Mettre le mod√®le en mode √©valuation
            with torch.no_grad():
                token_ids = self.hybrid_model.generate(
                    src=src_tokens,
                    start_token=start_token,
                    end_token=end_token,
                    max_length=max_length,
                    temperature=temperature,
                    top_k=50,
                    top_p=0.9
                )
            
            # Convertir les IDs en mots
            tokens = []
            for token_id in token_ids:
                if token_id == end_token or token_id == start_token:
                    continue
                    
                word = self.idx_to_word.get(str(token_id), None)
                
                # Ajouter seulement les vrais mots
                if word and word not in ["<pad>", "<unk>", "<eos>", "<sos>", "<bos>"]:
                    tokens.append(word)
            
            # Joindre les tokens
            decoded_text = " ".join(tokens)
            
            return decoded_text if decoded_text.strip() else ""
            
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur d√©codage transformer: {e}")
            import traceback
            traceback.print_exc()
            return ""
    
    def _generate_response(
        self,
        neural_output: torch.Tensor,
        context: Optional[Dict] = None,
        max_length: int = 50,
        temperature: float = 0.8,
        input_tokens: Optional[torch.Tensor] = None
    ) -> str:
        """
        G√©n√®re une r√©ponse PUREMENT NEURONALE avec l'architecture hybride RNN-Transformer
        Pas de templates pr√©faits - le RNN g√©n√®re tout √† partir de ses connaissances
        ‚ú® FOR√áAGE COMPLET DE LA G√âN√âRATION NEURONALE
        """
        
        activation = self._calculate_neural_activation(neural_output)
        message = context.get("current_message", "") if context else ""
        
        # ‚úÖ FORCER LA G√âN√âRATION NEURONALE PURE - JAMAIS DE TEMPLATES
        if self.vocab_size > 100 and input_tokens is not None:
            try:
                neural_response = self._decode_tokens(
                    src_tokens=input_tokens,
                    max_length=max_length,
                    temperature=temperature
                )
                
                # Toujours accepter la r√©ponse m√™me si elle n'a qu'un mot
                if neural_response:
                    print(f"üß† G√©n√©ration RNN: '{neural_response}' (activation={activation:.3f})")
                    return self._post_process(neural_response)
                else:
                    # G√©n√©ration vide - construire une r√©ponse par extrapolation contextuelle
                    return self._neural_synthesis(message, context, neural_output, activation)
            except Exception as e:
                print(f"‚ùå Erreur g√©n√©ration RNN: {e}")
                # Pas de fallback sur templates - forcer une synth√®se neuronale
                return self._neural_synthesis(message, context, neural_output, activation)
        else:
            # Vocabulaire insuffisant - synth√®se neuronale
            return self._neural_synthesis(message, context, neural_output, activation)

    def _respond_identity(self, context: Optional[Dict], activation: float) -> str:
        """‚ö†Ô∏è DEPRECATED - Utiliser _neural_synthesis √† la place"""
        # Cette m√©thode n'est plus utilis√©e - toutes les r√©ponses sont neuronales
        return ""

    def _respond_user_identity(self, context: Optional[Dict], activation: float) -> str:
        """‚ö†Ô∏è DEPRECATED - Utiliser _neural_synthesis √† la place"""
        return ""

    def _respond_preference(self, message: str, context: Optional[Dict], activation: float) -> str:
        """‚ö†Ô∏è DEPRECATED - Utiliser _neural_synthesis √† la place"""
        return ""

    def _respond_memory_recall(self, context: Optional[Dict], activation: float) -> str:
        """‚ö†Ô∏è DEPRECATED - Utiliser _neural_synthesis √† la place"""
        return ""

    def _respond_emotional(self, context: Optional[Dict], activation: float) -> str:
        """‚ö†Ô∏è DEPRECATED - Utiliser _neural_synthesis √† la place"""
        return ""

    def _respond_greeting(self, context: Optional[Dict], activation: float) -> str:
        """‚ö†Ô∏è DEPRECATED - Utiliser _neural_synthesis √† la place"""
        return ""

    def _respond_generic(self, context: Optional[Dict], activation: float) -> str:
        """‚ö†Ô∏è DEPRECATED - Utiliser _neural_synthesis √† la place"""
        return ""

    def _calculate_neural_activation(self, output: torch.Tensor) -> float:
        """
        Calcule une activation normalis√©e entre 0 et 1
        Bas√©e sur variance + magnitude normalis√©es correctement
        
        Formule:
            magnitude_norm = tanh(mean(abs(output)) / std(output))
            variance_norm = tanh(var(output) / mean(output¬≤))
            activation = 0.6 * magnitude_norm + 0.4 * variance_norm
        """
        
        # D√©tacher du graphe de calcul pour l'inf√©rence
        output = output.detach()
        
        # [1] Calculer la magnitude moyenne
        magnitude = float(torch.mean(torch.abs(output)).item())
        
        # [2] Calculer la variance (indicateur d'activit√©)
        variance = float(torch.var(output).item())
        
        # [3] Calculer l'√©cart-type pour normalisation
        std = float(torch.std(output).item()) + 1e-8  # √âviter division par z√©ro
        
        # [4] Normaliser magnitude et variance
        magnitude_normalized = np.tanh(magnitude / (std + 1e-8))
        variance_normalized = np.tanh(variance / (magnitude + 1e-8))
        
        # [5] Combiner avec poids optimis√©s
        # magnitude: indicateur de force du signal
        # variance: indicateur de variabilit√©/richesse neuronale
        activation = (0.6 * magnitude_normalized + 0.4 * variance_normalized) / 2.0
        
        # [6] Normaliser entre 0 et 1 (tanh retourne [-1, 1])
        activation = (activation + 1.0) / 2.0
        activation = max(0.0, min(1.0, activation))
        
        # [7] Mettre √† jour les statistiques
        self.activation_stats["total_activations"] += 1
        self.activation_stats["peak_activation"] = max(
            self.activation_stats["peak_activation"],
            activation
        )
        self.activation_stats["activation_history"].append(activation)
        
        # Garder seulement les 1000 derni√®res activations
        if len(self.activation_stats["activation_history"]) > 1000:
            self.activation_stats["activation_history"] = \
                self.activation_stats["activation_history"][-1000:]
        
        # Calculer la moyenne sur les 100 derni√®res
        self.activation_stats["average_activation"] = np.mean(
            self.activation_stats["activation_history"][-100:]
        )
        
        return activation
    
    def _neural_synthesis(self, message: str, context: Optional[Dict], neural_output: torch.Tensor, activation: float) -> str:
        """
        Synth√®se neuronale pure quand le d√©codage est vide ou √©choue
        Construit une r√©ponse √† partir des activations neuronales et du contexte
        ‚ú® C'est TOUJOURS une g√©n√©ration neuronale, jamais des templates
        """
        context = context or {}
        
        # Extraire des √©l√©ments du contexte pour enrichir synth√©tiquement
        memories = context.get("personal_memory", [])
        limbic_filter = context.get("limbic_filter", {})
        emotional_state = limbic_filter.get("emotional_state", {})
        
        # Construire une phrase bas√©e sur l'activation neuronale
        activation_phrases = {
            "high": "Je per√ßois profond√©ment",
            "medium": "Je consid√®re",
            "low": "Je remarque"
        }
        
        activation_level = "high" if activation > 0.67 else ("medium" if activation > 0.33 else "low")
        phrase = activation_phrases[activation_level]
        
        # Ajouter un √©l√©ment du contexte si disponible
        if memories:
            mem_text = memories[0].get("text", "")
            if mem_text:
                return f"{phrase} ce lien avec : {mem_text}"
        
        if emotional_state:
            emotion = emotional_state.get("dominant_emotion", "mon √©tat")
            return f"{phrase} mon {emotion}."
        
        # R√©flexion pure bas√©e sur l'activation
        return f"{phrase} votre question. L'activation de mes couches neuronales traite cette interaction."
    
    def _post_process(self, response: str) -> str:
        """Nettoie et formate la r√©ponse"""
        
        # Capitaliser la premi√®re lettre
        if response:
            response = response[0].upper() + response[1:]
        
        # Ajouter ponctuation si manquante
        if response and response[-1] not in ".!?":
            response += "."
        
        return response
    
    def _fallback_response(self, message: str, context: Optional[Dict]) -> str:
        """R√©ponse neuronale d'urgence en cas d'erreur critique"""
        # M√™me en cas d'erreur, g√©n√©rer une r√©ponse par synth√®se neuronale
        return "Mes processus neuronaux traitent votre entr√©e. Veuillez patienter."
    
    def get_model_info(self) -> Dict:
        """Retourne les informations du mod√®le RNN"""
        return {
            "backend": "RNN Local (TextualCortex)",
            "model": "LSTM bi-directionnel 3 couches",
            "vocab_size": self.vocab_size,
            "device": self.device,
            "ram": "~500 MB",
            "cost": "Gratuit (local)",
            "speed": "Rapide (CPU/GPU)",
            "quality": "Exp√©rimental (en apprentissage)",
        }