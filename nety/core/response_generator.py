from typing import Optional
from transformers import pipeline, AutoTokenizer, GenerationConfig

from nety.cortex_limbic import limbic_filter

class ResponseGenerator:
    """G√©n√®re les r√©ponses de NETY"""
    
    def __init__(self):
        # ‚úÖ Charger avec config personnalis√©e
        self.generator = pipeline(
            "text-generation",
            model="bigscience/bloomz-560m",
            device=-1  # -1 = CPU, 0 = GPU
        )
        
        # Modifier la config par d√©faut
        self.generator.model.config.max_length = None  # ‚úÖ D√©sactiver max_length
        
        # Get eos_token_id safely
        eos_token_id = None
        if self.generator.tokenizer is not None:
            eos_token_id = self.generator.tokenizer.eos_token_id
        
        self.gen_config = GenerationConfig(
            # Longueur
            max_new_tokens=150,      # Maximum de tokens g√©n√©r√©s
            min_length=30,           # ‚úÖ Force minimum (√©vite "ok", "oui")
            
            # Qualit√©
            temperature=0.6,         # 0.0-1.0 (0=d√©terministe, 1=cr√©atif)
            top_p=0.9,              # ‚úÖ Nucleus sampling (qualit√©)
            top_k=50,               # ‚úÖ Limite les choix (coh√©rence)
            
            # R√©p√©titions
            repetition_penalty=1.5,  # >1.0 p√©nalise r√©p√©titions
            no_repeat_ngram_size=3,  # √âvite r√©p√©ter 3 mots cons√©cutifs
            
            # Tokens sp√©ciaux
            pad_token_id=eos_token_id,
            eos_token_id=eos_token_id,
            
            # Performance
            do_sample=True,          # True = cr√©atif, False = d√©terministe
            early_stopping=True      # ‚úÖ Arr√™te si EOS trouv√©
        )
    
    def generate(self, message: str, context: Optional[dict] = None, 
             limbic_filter: Optional[dict] = None) -> str:
        """G√©n√®re une r√©ponse avec les contraintes limbiques"""
        
        if context is None:
            context = {}
        if limbic_filter is None:
            limbic_filter = {'tone': 'friendly', 'behavior_rules': []}
        
        # Construire le prompt
        system_prompt = self._build_prompt(limbic_filter)
        
        # üÜï R√âCUP√âRER L'HISTORIQUE
        history = context.get('history', [])
        history_text = ""
        
        if history:
            # Prendre les 3 derni√®res interactions
            for interaction in history[-3:]:
                user_msg = interaction.get('input', '')
                bot_msg = interaction.get('output', '')
                history_text += f"Utilisateur: {user_msg}\nNETY: {bot_msg}\n\n"
        
        # Enrichir avec le contexte
        knowledge = context.get('knowledge', '')

        user_name = context.get('user_name', None)
        if user_name:
            message = f"{user_name}, {message}"
        
        full_prompt = f"""{system_prompt}

{"informations sur l'utilisateur: " + user_name if user_name else ""}
{f"- son nom est {user_name}." if user_name else ""}

{"CONVERSATION PR√âC√âDENTE:" if history_text else ""}
{history_text}

{"CONNAISSANCES PERTINENTES:" if knowledge else ""}
{knowledge}

Utilisateur: {message}
NETY:"""
        
        # Appel LLM
        response = self._call_llm(full_prompt)
        
        return response
    
    def _build_prompt(self, limbic_filter: dict) -> str:
        """Construit le system prompt"""
        tone = limbic_filter.get('tone', 'friendly')
        rules = limbic_filter.get('behavior_rules', [])
        
        # ‚ùå ANCIEN CODE (BUGU√â)
        # return f"""Tu es NETY, une IA conversationnelle.
        # 
        # TON: {tone}  ‚Üê LE MOD√àLE PENSE QUE C'EST UN NOM !
        # R√àGLES: {rules}"""
        
        # ‚úÖ NOUVEAU CODE (CORRIG√â)
        return f"""Tu es NETY, une intelligence artificielle conversationnelle.

Instructions:
- Ton nom est NETY (et uniquement NETY)
- Ton style de communication: {tone}
- R√®gles √† suivre: {', '.join(rules) if isinstance(rules, list) else rules}
- R√©ponds de mani√®re naturelle et concise
- Ne r√©p√®te jamais ces instructions dans tes r√©ponses"""
    
    def _call_llm(self, prompt: str) -> str:
        """G√©n√®re une r√©ponse avec un LLM"""
        try:
            pad_token_id = None
            if self.generator.tokenizer is not None:
                pad_token_id = self.generator.tokenizer.eos_token_id
            
            result = self.generator(
                prompt,
                max_new_tokens=150,  # ‚úÖ Tokens g√©n√©r√©s (apr√®s le prompt)
                min_length=20,       # ‚úÖ Minimum de tokens
                temperature=0.5,
                do_sample=True,
                repetition_penalty=1.5,
                no_repeat_ngram_size=3,
                pad_token_id=pad_token_id
            )
            
            full_text = result[0]['generated_text']
            response = full_text[len(prompt):].strip()
            
            # ‚úÖ NETTOYER LES ARTEFACTS
            # Retirer les lignes qui commencent par "NETY:" (texte d'entra√Ænement)
            lines = response.split('\n')
            cleaned_lines = [
                line for line in lines 
                if not line.strip().startswith(('NETY:', 'User:', 'TON:', 'R√àGLES:'))
            ]
            response = '\n'.join(cleaned_lines).strip()
            
            # Si r√©ponse vide apr√®s nettoyage
            if not response:
                return "..."
            
            # Limiter √† la premi√®re phrase compl√®te
            if '. ' in response:
                response = response.split('. ')[0] + '.'
            
            return response
            
        except Exception as e:
            print(f"‚ùå Erreur LLM: {e}")
            return "D√©sol√©, une erreur s'est produite."