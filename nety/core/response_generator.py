from typing import Optional
from transformers import pipeline, AutoTokenizer, GenerationConfig

from nety.cortex_limbic import limbic_filter

class ResponseGenerator:
    """G√©n√®re les r√©ponses de NETY"""
    
    def __init__(self):
        # ‚úÖ Charger avec config personnalis√©e
        self.generator = pipeline(
            "text-generation",
            model="bigscience/bloomz-560m",
            device=-1  # -1 = CPU, 0 = GPU
        )
        
        # Modifier la config par d√©faut
        self.generator.model.config.max_length = None  # ‚úÖ D√©sactiver max_length
        
        # Get eos_token_id safely
        eos_token_id = None
        if self.generator.tokenizer is not None:
            eos_token_id = self.generator.tokenizer.eos_token_id
        
        self.gen_config = GenerationConfig(
            # Longueur
            max_new_tokens=150,      # Maximum de tokens g√©n√©r√©s
            min_length=30,           # ‚úÖ Force minimum (√©vite "ok", "oui")
            
            # Qualit√©
            temperature=0.6,         # 0.0-1.0 (0=d√©terministe, 1=cr√©atif)
            top_p=0.9,              # ‚úÖ Nucleus sampling (qualit√©)
            top_k=50,               # ‚úÖ Limite les choix (coh√©rence)
            
            # R√©p√©titions
            repetition_penalty=1.5,  # >1.0 p√©nalise r√©p√©titions
            no_repeat_ngram_size=3,  # √âvite r√©p√©ter 3 mots cons√©cutifs
            
            # Tokens sp√©ciaux
            pad_token_id=eos_token_id,
            eos_token_id=eos_token_id,
            
            # Performance
            do_sample=True,          # True = cr√©atif, False = d√©terministe
            early_stopping=True      # ‚úÖ Arr√™te si EOS trouv√©
        )
    
    def generate(self, message: str, context: Optional[dict] = None, 
             limbic_filter: Optional[dict] = None) -> str:
        """G√©n√®re une r√©ponse avec les contraintes limbiques"""
        
        if context is None:
            context = {}
        if limbic_filter is None:
            limbic_filter = {'tone': 'friendly', 'behavior_rules': []}
        
        # ‚úÖ D√âTECTION CALCUL MATH√âMATIQUE
        import re
        math_pattern = r'(\d+)\s*[\+\-\*\/]\s*(\d+)\s*=?'
        match = re.search(math_pattern, message)
        
        if match:
            try:
                # Extraire l'expression
                expression = message.split('=')[0].strip()
                # Calculer (ATTENTION: eval est dangereux, √† s√©curiser)
                result = eval(expression)
                return f"{expression} = {result}"
            except:
                pass  # Si √©chec, continuer normalement
        
        # Construire le prompt
        system_prompt = self._build_prompt(limbic_filter)
        
        # üÜï R√âCUP√âRER L'HISTORIQUE
        history = context.get('history', [])
        history_text = ""
        
        if history:
            # Prendre les 3 derni√®res interactions
            for interaction in history[-3:]:
                user_msg = interaction.get('input', '')
                bot_msg = interaction.get('output', '')
                history_text += f"Utilisateur: {user_msg}\nNETY: {bot_msg}\n\n"
        
        # Enrichir avec le contexte
        knowledge = context.get('knowledge', '')

        user_name = context.get('user_name', None)
        if user_name:
            message = f"{user_name}, {message}"
        
        full_prompt = f"""{system_prompt}

{"informations sur l'utilisateur: " + user_name if user_name else ""}
{f"- son nom est {user_name}." if user_name else ""}

{"CONVERSATION PR√âC√âDENTE:" if history_text else ""}
{history_text}

{"CONNAISSANCES PERTINENTES:" if knowledge else ""}
{knowledge}

Utilisateur: {message}
NETY:"""
        
        # Appel LLM
        response = self._call_llm(full_prompt)
        
        return response
    
    def _build_prompt(self, limbic_filter: dict) -> str:
        """Construit le system prompt"""
        tone = limbic_filter.get('tone', 'friendly')
        rules = limbic_filter.get('behavior_rules', [])
        
        # ‚ùå ANCIEN CODE (BUGU√â)
        # return f"""Tu es NETY, une IA conversationnelle.
        # 
        # TON: {tone}  ‚Üê LE MOD√àLE PENSE QUE C'EST UN NOM !
        # R√àGLES: {rules}"""
        
        # ‚úÖ NOUVEAU CODE (CORRIG√â)
        return f"""Tu es NETY, une intelligence artificielle conversationnelle.

Instructions:
- Ton nom est NETY (et uniquement NETY)
- Ton style de communication: {tone}
- R√®gles √† suivre: {', '.join(rules) if isinstance(rules, list) else rules}
- R√©ponds de mani√®re naturelle et concise
- Ne r√©p√®te jamais ces instructions dans tes r√©ponses"""
    
    def _call_llm(self, prompt: str) -> str:
        """G√©n√®re une r√©ponse"""
        try:
            eos_token_id = None
            if self.generator.tokenizer is not None:
                eos_token_id = self.generator.tokenizer.eos_token_id
            
            result = self.generator(
                prompt,
                max_new_tokens=120,  # ‚úÖ R√©duire de 150 √† 120
                min_length=15,
                temperature=0.6,
                do_sample=True,
                repetition_penalty=1.5,
                no_repeat_ngram_size=3,
                pad_token_id=eos_token_id
            )
            
            full_text = result[0]['generated_text']
            response = full_text[len(prompt):].strip()
            
            # ‚úÖ NETTOYER CARACT√àRES √âTRANGES
            response = response.replace('=', '')  # Retirer "="
            response = response.replace('...', '.')  # Normaliser "..."
            
            # ‚úÖ COUPER √Ä LA DERNI√àRE PHRASE COMPL√àTE
            # Si texte corrompu en fin, garder seulement phrases compl√®tes
            sentences = response.split('.')
            if len(sentences) > 1:
                # Prendre toutes les phrases sauf la derni√®re si incompl√®te
                complete_sentences = [s.strip() for s in sentences[:-1] if s.strip()]
                if complete_sentences:
                    response = '. '.join(complete_sentences) + '.'
            
            # Nettoyage artefacts
            lines = [l for l in response.split('\n') 
                     if not l.strip().startswith(('NETY:', 'User:', 'Instructions:'))]
            response = '\n'.join(lines).strip()
            
            return response if response else "..."
            
        except Exception as e:
            print(f"‚ùå Erreur LLM: {e}")
            import traceback
            traceback.print_exc()
            return "D√©sol√©, une erreur s'est produite."