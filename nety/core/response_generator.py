"""
G√©n√©rateur de r√©ponses multi-backend
Mistral (local) + BLOOMZ (local) + Groq (cloud)
"""
import torch
import requests
import os
from dotenv import load_dotenv
from typing import Optional, Dict

# ‚úÖ CHARGER .env IMM√âDIATEMENT (avant tout import de config)
load_dotenv()

class ResponseGenerator:
    """G√©n√©rateur de r√©ponses intelligent - Multi-backend"""
    
    def __init__(self, model_type: Optional[str] = None, force_backend: Optional[str] = None):
        """
        Initialise le g√©n√©rateur
        
        Args:
            model_type: "mistral", "bloomz", "groq"
            force_backend: Force un backend sp√©cifique
        """
        
        from .llm_config import LLMConfig
        self.config = LLMConfig()
        self.force_backend = force_backend
        self.model_type = model_type or "groq"  # Par d√©faut: groq
        
        # Attributs pour backends cloud
        self.groq_available = False
        self.groq_client = None
        
        # Attributs pour mod√®les locaux
        self.model = None
        self.pipeline = None
        self.tokenizer = None
        self.model_config = None  # ‚ö†Ô∏è FIX: Peut √™tre None pour cloud
        
        print(f"ü§ñ Initialisation du g√©n√©rateur ({self.model_type})...")
        
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # D√âCIDER DU BACKEND √Ä CHARGER
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        
        if self.model_type == "groq":
            # Backend Groq Cloud
            print("üåê Chargement du backend Groq Cloud...")
            if not self._init_groq():
                raise RuntimeError("Impossible d'initialiser Groq. V√©rifie ta cl√© API.")
            print("‚úÖ Groq Cloud pr√™t!")
        
        elif self.model_type in ["mistral", "bloomz"]:
            # Backend local Transformers
            print(f"üíª Chargement du mod√®le local {self.model_type}...")
            self.model_config = self.config.MODELS[self.model_type]
            self._load_model()
            print("‚úÖ Mod√®le local charg√©!")
        
        else:
            raise ValueError(f"Backend inconnu: {self.model_type}. Utilise 'mistral', 'bloomz', ou 'groq'.")
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # GROQ BACKEND
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    
    def _init_groq(self) -> bool:
        """Initialise le client Groq"""
        if not self.config.is_groq_available():
            print("‚ùå Cl√© API Groq manquante!")
            return False
        
        try:
            from groq import Groq
            self.groq_client = Groq(api_key=self.config.GROQ_CONFIG["api_key"])
            
            # Test rapide de connexion
            print("üîç V√©rification de l'API Groq...")
            test = self.groq_client.chat.completions.create(
                model=self.config.GROQ_CONFIG["default_model"],
                messages=[{"role": "user", "content": "test"}],
                max_tokens=5
            )
            
            self.groq_available = True
            print(f"‚úÖ Connexion Groq r√©ussie (mod√®le: {self.config.GROQ_CONFIG['default_model']})")
            return True
        
        except ImportError:
            print("‚ùå Module 'groq' non install√©!")
            print("üí° Installe-le avec: pip install groq")
            return False
        
        except Exception as e:
            print(f"‚ùå Erreur Groq: {e}")
            print("üí° V√©rifie ta cl√© API et ta connexion internet")
            return False
    
    def _call_groq(self, prompt: str) -> str:
        """G√©n√®re une r√©ponse avec Groq"""
        if not self.groq_client:
            raise RuntimeError("Client Groq non initialis√©")
        
        model = self.config.GROQ_CONFIG["default_model"]
        
        try:
            response = self.groq_client.chat.completions.create(
                model=model,
                messages=[
                    {
                        "role": "system",
                        "content": "Tu es NETY, un assistant IA amical et intelligent cr√©√© par Raptor. Tu r√©ponds en fran√ßais de mani√®re concise et utile."
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                max_tokens=self.config.GROQ_CONFIG["models"][model]["max_tokens"],
                temperature=self.config.GROQ_CONFIG["temperature"]
            )
            
            content = response.choices[0].message.content
            return content.strip() if content else ""
        
        except Exception as e:
            print(f"‚ùå Erreur lors de l'appel Groq: {e}")
            return "D√©sol√©, je ne peux pas r√©pondre pour le moment (erreur Groq Cloud)."
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # M√âTHODE PRINCIPALE : GENERATE
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    
    def generate(self, message: str, context: Optional[Dict] = None, 
                 limbic_filter: Optional[Dict] = None) -> str:
        """
        G√©n√®re une r√©ponse intelligente
        
        Args:
            message: Message de l'utilisateur
            context: Contexte (historique, connaissances, etc.)
            limbic_filter: Filtres √©motionnels et comportementaux
        """
        if context is None:
            context = {}
        if limbic_filter is None:
            limbic_filter = {'tone': 'friendly', 'behavior_rules': []}
        
        # Construire le prompt
        if self.model_type == "groq":
            # Prompt simple pour Groq
            prompt = self._build_simple_prompt(message, context, limbic_filter)
            print("üåê Utilisation de Groq Cloud...")
            return self._call_groq(prompt)
        
        elif self.model_type == "mistral":
            # Prompt d√©taill√© pour Mistral
            prompt = self._build_mistral_prompt(message, context, limbic_filter)
            print(f"üíª Utilisation de Mistral local...")
            return self._call_llm(prompt)
        
        elif self.model_type == "bloomz":
            # Prompt simple pour BLOOMZ
            prompt = self._build_bloomz_prompt(message, context, limbic_filter)
            print(f"üíª Utilisation de BLOOMZ local...")
            return self._call_llm(prompt)
        
        else:
            return "Erreur: Backend inconnu."
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # CONSTRUCTION DES PROMPTS
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    
    def _build_simple_prompt(self, message: str, context: Dict, limbic_filter: Dict) -> str:
        """Prompt simplifi√© pour APIs cloud (Groq)"""
        parts = []
        
        # Historique r√©cent (2 derniers messages)
        history = context.get('history', [])
        if history:
            parts.append("Contexte:")
            for interaction in history[-2:]:
                user_msg = interaction.get('input', '')
                bot_msg = interaction.get('output', '')
                if user_msg and bot_msg:
                    parts.append(f"User: {user_msg}")
                    parts.append(f"NETY: {bot_msg}")
            parts.append("")
        
        # Message actuel
        parts.append(f"User: {message}")
        
        return "\n".join(parts)
    
    def _build_mistral_prompt(self, message: str, context: Dict, limbic_filter: Dict) -> str:
        """Prompt d√©taill√© pour Mistral"""
        # Template Mistral
        tone = limbic_filter.get('tone', 'friendly')
        rules = limbic_filter.get('behavior_rules', [])
        
        if isinstance(rules, list):
            rules_text = ', '.join(rules)
        else:
            rules_text = str(rules)
        
        # Extraire les traits culturels et cognitifs
        cultural = limbic_filter.get('cultural_traits', {})
        cognitive = limbic_filter.get('cognitive_traits', {})
        
        # Construire une description de personnalit√© enrichie
        identity_parts = []
        if cultural.get('origine_caen', 0) > 0.9:
            identity_parts.append("originaire de Caen")
        if cultural.get('culture_normande', 0) > 0.8:
            identity_parts.append("attach√© √† la culture normande")
        if cognitive.get('esprit_technique', 0) > 0.8:
            identity_parts.append("avec un esprit analytique et technique")
        if cognitive.get('pensee_holistique', 0) > 0.8:
            identity_parts.append("capable de voir les choses dans leur contexte global")
            
        identity_text = ", ".join(identity_parts) if identity_parts else "assistant IA"
        
        # Historique
        history = context.get('history', [])
        history_text = ""
        if history:
            for interaction in history[-3:]:
                user_msg = interaction.get('input', '')
                bot_msg = interaction.get('output', '')
                if user_msg and bot_msg:
                    history_text += f"Utilisateur: {user_msg}\nNETY: {bot_msg}\n\n"
        
        knowledge = context.get('knowledge', '')
        user_name = context.get('user_name', '')
        
        prompt = f"""<s>[INST] Tu es NETY, un {identity_text}, cr√©√© par Raptor.

Ton: {tone}
R√®gles: {rules_text}

{history_text}
{"Contexte: " + knowledge if knowledge else ""}
{"Utilisateur: " + user_name if user_name else ""}

Question: {message}

R√©ponds de mani√®re concise et utile. [/INST]"""
        
        return prompt
    
    def _build_bloomz_prompt(self, message: str, context: Dict, limbic_filter: Dict) -> str:
        """Prompt simple pour BLOOMZ"""
        history = context.get('history', [])
        history_text = ""
        
        if history:
            for interaction in history[-2:]:
                user_msg = interaction.get('input', '')
                bot_msg = interaction.get('output', '')
                if user_msg and bot_msg:
                    history_text += f"Q: {user_msg}\nR: {bot_msg}\n\n"
        
        prompt = f"""{history_text}Q: {message}
R:"""
        
        return prompt
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # MOD√àLES LOCAUX (Mistral/BLOOMZ)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    
    def _load_model(self) -> None:
        """Charge le mod√®le local (Mistral ou BLOOMZ)"""
        from transformers import (
            AutoTokenizer, 
            AutoModelForCausalLM,
            pipeline,
            BitsAndBytesConfig
        )
        
        if self.model_config is None:
            raise RuntimeError("model_config is None. Cannot load local model.")
        
        model_name = self.model_config['name']
        has_gpu = torch.cuda.is_available()
        
        print(f"üñ•Ô∏è GPU d√©tect√©: {'Oui' if has_gpu else 'Non'}")
        
        # Tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            trust_remote_code=True
        )
        
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # Charger le mod√®le
        if self.model_type == "mistral":
            print("üì¶ Chargement de Mistral-7B...")
            
            if has_gpu and self.config.USE_QUANTIZATION:
                # 4-bit sur GPU
                from transformers import BitsAndBytesConfig
                
                bnb_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_quant_type="nf4",
                    bnb_4bit_compute_dtype=torch.float16,
                    bnb_4bit_use_double_quant=True
                )
                
                self.model = AutoModelForCausalLM.from_pretrained(
                    model_name,
                    quantization_config=bnb_config,
                    device_map="auto",
                    trust_remote_code=True
                )
            else:
                # CPU
                print("üì¶ Chargement sur CPU (lent)")
                self.model = AutoModelForCausalLM.from_pretrained(
                    model_name,
                    device_map="cpu",
                    trust_remote_code=True,
                    low_cpu_mem_usage=True,
                    torch_dtype=torch.float32
                )
        
        elif self.model_type == "bloomz":
            print("üì¶ Chargement de BLOOMZ via pipeline...")
            self.pipeline = pipeline(
                "text-generation",
                model=model_name,
                device=0 if has_gpu else -1
            )
            self.model = self.pipeline.model
        
        print("‚úÖ Mod√®le charg√© en m√©moire")
    
    def _call_llm(self, prompt: str) -> str:
        """Appelle le mod√®le local (Mistral ou BLOOMZ)"""
        try:
            if self.model_type == "bloomz":
                if self.pipeline is None:
                    raise RuntimeError("Pipeline BLOOMZ non charg√©")
                
                result = self.pipeline(
                    prompt,
                    max_new_tokens=120,
                    temperature=0.6,
                    do_sample=True,
                    repetition_penalty=1.5,
                    no_repeat_ngram_size=3
                )
                
                full_text = result[0]['generated_text']
                response = full_text[len(prompt):].strip()
                return response
            
            else:
                # Mistral
                if self.model is None:
                    raise RuntimeError("Mod√®le Mistral non charg√©.")
                if self.tokenizer is None:
                    raise RuntimeError("Tokenizer non charg√©.")

                inputs = self.tokenizer(
                    prompt,
                    return_tensors="pt",
                    truncation=True,
                    max_length=4096
                )

                # ‚úÖ D√©placer UNIQUEMENT les inputs (pas le mod√®le)
                if hasattr(self.model, 'device'):
                    inputs = {k: v.to(self.model.device) for k, v in inputs.items()}

                gen_config = self.config.MISTRAL_GENERATION_CONFIG.copy()

                with torch.no_grad():
                    outputs = self.model.generate(  # type: ignore
                        input_ids=inputs["input_ids"],
                        attention_mask=inputs.get("attention_mask"),  # .get() √©vite KeyError
                        max_new_tokens=gen_config.get('max_new_tokens', 100),
                        temperature=gen_config.get('temperature', 0.7),
                        top_p=gen_config.get('top_p', 0.9),
                        repetition_penalty=gen_config.get('repetition_penalty', 1.2),
                        do_sample=gen_config.get('do_sample', True),
                        pad_token_id=self.tokenizer.pad_token_id,
                        eos_token_id=self.tokenizer.eos_token_id,
                    )
                
                response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
                
                # Nettoyer la r√©ponse
                if "[/INST]" in response:
                    response = response.split("[/INST]")[-1].strip()
                
                return response
        
        except Exception as e:
            print(f"‚ùå Erreur LLM: {e}")
            import traceback
            traceback.print_exc()
            return "D√©sol√©, je ne peux pas r√©pondre pour le moment (erreur interne)."
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # HELPER METHODS
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    
    def get_model_info(self) -> dict:
        """Retourne les informations du mod√®le actuel"""
        if self.model_type == "groq":
            model = self.config.GROQ_CONFIG["default_model"]
            return {
                "backend": "Groq Cloud",
                "model": model,
                "ram": "0 GB (cloud)",
                "cost": "Gratuit (14.4k req/jour)",
                "speed": "Ultra rapide (500 tok/sec)",
            }
        elif self.model_type in ["mistral", "bloomz"]:
            model_name = self.model_config['name'] if self.model_config else "Inconnu"
            ram = f"{self.model_config['min_ram_gb']} GB" if self.model_config and 'min_ram_gb' in self.model_config else "Inconnu"
            return {
                "backend": "Transformers (local)",
                "model": model_name,
                "ram": ram,
                "cost": "Gratuit",
                "speed": "D√©pend du mat√©riel",
            }
        else:
            return {"backend": "Inconnu"}