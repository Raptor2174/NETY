from typing import Optional
from transformers import pipeline

class ResponseGenerator:
    """G√©n√®re les r√©ponses de NETY"""
    
    def __init__(self):
        # Charger le mod√®le une seule fois
        self.generator = pipeline(
            "text-generation", 
            model="bigscience/bloomz-560m"
        )
    
    def generate(self, message: str, context: Optional[dict] = None, 
             limbic_filter: Optional[dict] = None) -> str:
        """G√©n√®re une r√©ponse avec les contraintes limbiques"""
        
        if context is None:
            context = {}
        if limbic_filter is None:
            limbic_filter = {'tone': 'friendly', 'behavior_rules': []}
        
        # Construire le prompt
        system_prompt = self._build_prompt(limbic_filter)
        
        # üÜï R√âCUP√âRER L'HISTORIQUE
        history = context.get('history', [])
        history_text = ""
        
        if history:
            # Prendre les 3 derni√®res interactions
            for interaction in history[-3:]:
                user_msg = interaction.get('input', '')
                bot_msg = interaction.get('output', '')
                history_text += f"Utilisateur: {user_msg}\nNETY: {bot_msg}\n\n"
        
        # Enrichir avec le contexte
        knowledge = context.get('knowledge', '')
        
        full_prompt = f"""{system_prompt}

{"CONVERSATION PR√âC√âDENTE:" if history_text else ""}
{history_text}

{"CONNAISSANCES PERTINENTES:" if knowledge else ""}
{knowledge}

Utilisateur: {message}
NETY:"""
        
        # Appel LLM
        response = self._call_llm(full_prompt)
        
        return response
    
    def _build_prompt(self, limbic_filter: dict) -> str:
        """Construit le system prompt"""
        tone = limbic_filter.get('tone', 'friendly')
        rules = limbic_filter.get('behavior_rules', [])
        
        return f"""Tu es NETY, une IA conversationnelle.

TON: {tone}
R√àGLES: {rules}"""
    
    def _call_llm(self, prompt: str) -> str:
        """Appelle le LLM"""
        try:
            result = self.generator(
                prompt,
                max_new_tokens=100,
                temperature=0.7,
                do_sample=True,
                pad_token_id=getattr(getattr(self.generator, "tokenizer", None), "eos_token_id", None)
            )
            
            # Retirer le prompt de la r√©ponse compl√®te
            full_text = result[0]['generated_text']
            response = full_text[len(prompt):].strip()
            
            return response if response else "..."
            
        except Exception as e:
            print(f"‚ùå Erreur LLM: {e}")
            return "Erreur de g√©n√©ration."
        

    